Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Baron2016,
address = {York, United Kingdom},
author = {Baron, Grzegorz},
booktitle = {Procedia Computer Science 96 ( 2016 ) : 20th International Conference on Knowledge Based and Intelligent Information and Engineering Systems, KES2016},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/On influence of representations of discretized data on performance.pdf:pdf},
pages = {1418--1427},
title = {{On influence of representations of discretized data on performance of a decision system}},
year = {2016}
}
@book{Wu2008,
abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, k NN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development. {\textcopyright} Springer-Verlag London Limited 2007. },
author = {Wu, Xindong and Kumar, Vipin and Ross, Quinlan J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
booktitle = {Knowledge and Information Systems},
doi = {10.1007/s10115-007-0114-2},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/10Algorithms-08.pdf:pdf},
isbn = {1011500701},
issn = {02191377},
number = {1},
pages = {1--37},
pmid = {25720841},
title = {{Top 10 algorithms in data mining}},
volume = {14},
year = {2008}
}
@article{Morasca2002,
abstract = {Classification trees have been successfully used in several application fields. However, continuous attributes cannot be used directly when building classification trees, but they must be first discretized with clustering techniques, which require some degree of subjectivity. We propose an approach to build classification trees that does not require the discretization of the continuous attributes. The approach is an extension of existing methods for building classification trees and is based on the information gain yielded by discrete and continuous attributes. Data from a software development case study are analyzed with both the proposed approach and C4.5 to show the approach's applicability and benefits over C4.5. Copyright 2002 ACM.},
author = {Morasca, Sandro and Chimiche, Scienze and Matematiche, Fisiche},
doi = {10.1145/568760.568832},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p417-morasca.pdf:pdf},
isbn = {1581135564},
keywords = {5,c4,continuous attributes,decision trees,id3},
pages = {0--7},
title = {{A Proposal for Using Continuous Attributes in Classification Trees}},
year = {2002}
}
@book{pedro2017,
address = {S{\~{a}}o Paulo - Brasil},
author = {Domingos, Pedro},
edition = {1a.},
editor = {Prates, Rubens},
isbn = {9788575225424},
publisher = {Novatec Editora Ltda},
title = {{O Algoritmo Mestre}},
year = {2017}
}
@article{Steiner2006,
abstract = {A “Descoberta de Conhecimento em Bases de Dados” (Knowledge Discovery in Databases, KDD) {\'{e}} um processo composto de v{\'{a}}rias etapas, iniciando com a coleta de dados para o problema em pauta e finalizando com a interpreta- {\c{c}}{\~{a}}o e avalia{\c{c}}{\~{a}}o dos resultados obtidos. O presente trabalho objetiva mostrar a influ{\^{e}}ncia da an{\'{a}}lise explorat{\'{o}}ria dos dados no desempenho das t{\'{e}}cnicas de Minera{\c{c}}{\~{a}}o de Dados (Data Mining) quanto {\`{a}} classifica{\c{c}}{\~{a}}o de novos padr{\~{o}}es por meio da sua aplica{\c{c}}{\~{a}}o a um problema m{\'{e}}dico, al{\'{e}}m de comparar o desempenho delas entre si, visando obter a t{\'{e}}cnica com o maior percentual de acertos. Pelos resultados obtidos, pode-se concluir que a referida an{\'{a}}lise, se conduzida de forma adequada, pode trazer importantes melhorias nos desempenhos de quase todas as t{\'{e}}cnicas abor- dadas, tornando-se, assim, uma importante ferramenta para a otimiza{\c{c}}{\~{a}}o dos resultados finais. Para o problema em estudo, a t{\'{e}}cnica que envolve um modelo de Programa{\c{c}}{\~{a}}o Linear e uma outra que envolve Redes Neurais foram as t{\'{e}}cnicas que apresentaram os menores percentuais de erros para os conjuntos de testes, apresentando capacidades de generaliza{\c{c}}{\~{a}}o satisfat{\'{o}}rias.},
author = {Steiner, Maria Teresinha Arns MTA and Soma, Nei Yoshihiro NY and Shimizu, Tamio and Nievola, J{\'{u}}lio Cesar and steiner Neto, Pedro Jos{\'{e}}},
doi = {10.1590/S0104-530X2006000200013},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/31177{\_}problema{\_}medico{\_}processo{\_}kdd{\_}analise{\_}exploratoria{\_}dados.pdf:pdf},
issn = {0104-530X},
journal = {Gest{\~{a}}o {\&} Produ{\c{c}}{\~{a}}o},
keywords = {an{\'{a}}lise explorat{\'{o}}ria dos dados.,minera{\c{c}}{\~{a}}o de dados,processo KDD},
pages = {325--337},
title = {{Abordagem de um problema m{\'{e}}dico por meio do processo de KDD com {\^{e}}nfase {\`{a}} an{\'{a}}lise explorat{\'{o}}ria dos dados.}},
url = {http://www.scielo.br/scielo.php?script=sci{\_}arttext{\&}pid=S0104-530X2006000200013{\&}nrm=iso{\%}5Cnhttp://www.scielo.br/pdf/gp/v13n2/31177.pdf},
volume = {13},
year = {2006}
}
@inproceedings{Yeganova2010a,
author = {Yeganova, Lana and Comeau, Donald C. and Wilbur, W. John},
booktitle = {2010 Ninth International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2010.166},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/identifying Abbreviation Definitions{\_}ML with Naturally Labeled data.pdf:pdf},
isbn = {978-1-4244-9211-4},
month = {dec},
pages = {499--505},
publisher = {IEEE},
title = {{Identifying Abbreviation Definitions Machine Learning with Naturally Labeled Data}},
url = {http://ieeexplore.ieee.org/document/5708877/},
year = {2010}
}
@techreport{Lachi2005,
abstract = {Clustering {\'{e}} uma forma de organizar dados por meio do agrupamento destes em conjuntos, a partir da maior similaridade existente entre os dados de um mesmo conjunto que os de outro, com base em algum crit{\'{e}}rio pr{\'{e}}- determinado. Neste relat{\'{o}}rio s{\~{a}}o apresentados os aspectos gerais que devem ser observados quando se pretende aplicar alguma t{\'{e}}cnica de clustering na resolu{\c{c}}{\~{a}}o de um problema. Os aspectos gerais apresentados s{\~{a}}o: defini{\c{c}}{\~{a}}o da forma de representa{\c{c}}{\~{a}}o do conjunto de dados a serem agrupados, defini{\c{c}}{\~{a}}o de uma medida adequada de semelhan{\c{c}}a entre os dados e a defini{\c{c}}{\~{a}}o de qual t{\'{e}}cnica de clustering utilizar para a constru{\c{c}}{\~{a}}o dos clusters. Al{\'{e}}m disso, uma farta quantidade de refer{\^{e}}ncias bibliogr{\'{a}}ficas {\'{e}} disponibilizada permitindo ao leitor o aprofundamento em todos os t{\'{o}}picos presentes},
address = {Campinas, SP},
author = {Lachi, Ricardo Lu{\'{i}}s and Rocha, Heloisa Vieira},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Aspectos basicos de clustering{\_} conceitos e tecnicas.pdf:pdf},
institution = {Instituto de Computa{\c{c}}{\~{a}}o – Universidade Estadual de Campinas},
isbn = {0008-8749 (Print) 0008-8749 (Linking)},
pages = {1--26},
pmid = {6968247},
title = {{Aspectos b{\'{a}}sicos de clustering: conceitos e t{\'{e}}cnicas}},
url = {http://www.ic.unicamp.br/{~}reltech/2005/05-03.pdf},
year = {2005}
}
@article{Chang2016,
author = {Chang, Shuo and Dai, Peng and Hong, Lichan and Sheng, Cheng and Zhang, Tianjiao and Chi, Ed H.},
doi = {10.1145/2856767.2856783},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p348-chang.pdf:pdf},
isbn = {9781450341370},
journal = {Proceedings of the 21st International Conference on Intelligent User Interfaces - IUI '16},
keywords = {clustering,interactive machine learning,search result clustering},
pages = {348--358},
title = {{AppGrouper: Knowledge-graph-based Interactive Clustering Tool for Mobile App Search Results}},
url = {http://dl.acm.org/citation.cfm?doid=2856767.2856783},
year = {2016}
}
@article{Lopes2016,
abstract = {Clustering is a major problem that, among other areas, involves ma- chine learning. As important as identifying the groups is providing a definition for them. The current work presents a model that uses a combination of algo- rithms with supervised and unsupervised learning, aiming the creation ofgroups and the identification ofwhich attributes may define them. Besides, this article presents some results ofthe executions applied in two different databases.},
author = {Lopes, Lucas A and Machado, Vinicius P and Rabelo, Ricardo De A L},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/artigo{\_}lopes.pdf:pdf},
journal = {Knowledge-Based Systems},
keywords = {cluster,label,machine learning},
mendeley-tags = {cluster,label,machine learning},
pages = {231--241},
title = {{Automatic Labeling of Groupings through Supervised Machine Learning}},
volume = {106},
year = {2016}
}
@article{Sanches2003,
abstract = {In order to apply machine learning classification algorithms, it is assumed that there exists a set of labeled data, called the training set, which is used to train the classifier. However, in real life, this training set may not contain enough labeled data to induce a good classifier. Recently, there has been much interest in a variant of this approach. This new approach, known as semi-supervised learning, assumes that, in addition to the labeled training data, there exists a second set of unlabeled data which is also available during training. One of the goals of semi-supervised learning is training classifiers when large amounts of unlabeled data are available together with a small amount of labeled data. The appeal of semi-supervised learning is due to the fact that in many real-world applications, such sets of unlabeled data are either readily available or inexpensive to collect compared to labeled data. Unlabeled data can often be collected by auto- mated means while labeled data requires human experts or other limited or expensive classification resources that may even be unfeasible in some cases. There are several ways in which unlabeled data can be used. In this work we explore one mechanism by which unlabeled data can be used to improve classification problems and propose a semi-supervised algorithm, called k-meanski, for using unlabeled data for supervised learning. There are two premises underlying the technique used by the proposed algorithm. The first is that input data falls naturally into clusters rather than being uniformly distributed across the entire input space. Furthermore, the initial input labeled data should fall near the center of the existing clusters in the input space. The second premise is that many of the points in some of these clusters belong to specific output categories. Obviously, the validity of these premises is dependent on the dataset used. The k-meanski algorithm works well when the data conform to both premises. If these assumptions are violated poor performance can result. Experiments using real world datasets where we randomly select a subset of the available data to act as labeled exemplars are shown.},
author = {Sanches, Marcelo Kaminski},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Dissertacao{\_}MKS.pdf:pdf},
title = {{Aprendizado de m{\'{a}}quina semi-supervisionado: proposta de um algoritmo para rotular exemplos a partir de poucos exemplos rotulados}},
year = {2003}
}
@misc{Metodo2015,
abstract = {Na tarefa de classifica{\c{c}}{\~{a}}o utilizando algoritmos de aprendizado de m{\'{a}}quina, considera-se a exist{\^{e}}ncia de uma base de dados chamada conjunto de treinamento. Esse conjunto possui exemplos que s{\~{a}}o rotulados(pr{\'{e}}-classificados) e utilizados no treinamento do classificador. Deve ter um total de exemplos significativo e equilibrado para que, ap{\'{o}}s o treinamento, o classificador tenha um desempenho satisfat{\'{o}}rio. Por{\'{e}}m, na maioria dos casos reais, obter esse conjunto de treinamento com a quantidade de exemplos suficientes para induzir um classificador no treinamento pode ser oneroso, pois {\'{e}} necess{\'{a}}rio que seja realizada uma rotula{\c{c}}{\~{a}}o dos dados por um especialista no problema em quest{\~{a}}o. Exemplos n{\~{a}}o-rotulados s{\~{a}}o mais f{\'{a}}ceis de serem coletados em compara{\c{c}}{\~{a}}o aos que possuem r{\'{o}}tulos. A literatura mostra o interesse da comunidade cient{\'{i}}fica em uma nova abordagem de aprendizado chamada de semissupervisionada. Este tipo de aprendizado trabalha em um cen{\'{a}}rio em que existe um conjunto de dados rotulados, insuficiente para treinar um classificador, juntamente com um outro conjunto com dados n{\~{a}}o-rotulados, tamb{\'{e}}m, dispon{\'{i}}vel no treinamento. O objetivo do trabalho {\'{e}} propor um m{\'{e}}todo que visa rotular dados a partir de um pequeno conjunto rotulado. Esse m{\'{e}}todo combina um classificador e um agrupador para realizar a tarefa de classifica{\c{c}}{\~{a}}o de forma simples em rela{\c{c}}{\~{a}}o {\`{a}} outros m{\'{e}}todos encontrados na literatura. Foram realizados experimentos utilizando 5 bases de dados e os resultados comparados com os algoritmos co-training e k-meanski, que s{\~{a}}o outros algoritmos},
address = {Teresina - PI},
author = {Lima, Bruno Vicente Alves},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Dissertacao{\_}Bruno{\_}VicenteUFPI.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}} - UFPI},
keywords = {Semissupervisionado. Classifica{\c{c}}{\~{a}}o. Rotula{\c{c}}{\~{a}}o. Apr},
mendeley-tags = {Semissupervisionado. Classifica{\c{c}}{\~{a}}o. Rotula{\c{c}}{\~{a}}o. Apr},
pages = {47},
title = {{M{\'{e}}todo Semissupervisionado de Rotula{\c{c}}{\~{a}}o e Classifica{\c{c}}{\~{a}}o Utilizando Agrupamento por Sementes e Classificadores}},
type = {Disserta{\c{c}}{\~{a}}o (Programa de P{\'{o}}s-gradua{\c{c}}{\~{a}}o em Ci{\^{e}}ncia da Computa{\c{c}}{\~{a}}o)},
year = {2015}
}
@article{Sun2011,
author = {Sun, Liang and Yoshida, Shinichi and Liang, Yanchun},
doi = {10.1587/transinf.E94.D.2234},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/A Novel Support Vector and K-Means based Hybrid clustering algorithm.pdf:pdf},
isbn = {9781424457021},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Data clustering,K-Means clustering,Kernel methods,Support vector clustering},
number = {11},
pages = {2234--2243},
title = {{A support vector and k-means based hybrid intelligent data clustering algorithm}},
volume = {E94-D},
year = {2011}
}
@article{Astafiev2018,
author = {Astafiev, A V},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Development of Automated Identification {\_} Not Typed Routes Using Multi.pdf:pdf},
isbn = {9781538649381},
journal = {2018 International Russian Automation Conference (RusAutoCon)},
keywords = {automated,multi-code labeling,multi-code labels},
pages = {1--4},
publisher = {IEEE},
title = {{Development of Automated Identification Technology Objects During Their Movement Along Not Typed Routes Using Multi-Code Labeling}},
year = {2018}
}
@article{Salazar2003,
author = {Salazar, Luiz Filipe},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/DISCRETIZACAO{\_}APRENDIZAGEM{\_}BAYESINA.pdf:pdf},
journal = {Ele.Ita.Br},
title = {{Discretiza{\c{c}}{\~{a}}o para Aprendizagem Bayesiana: Aplica{\c{c}}{\~{a}}o no Aux{\'{i}}lio {\`{a}} Valida{\c{c}}{\~{a}}o de Dados em Prote{\c{c}}{\~{a}}o ao V{\^{o}}o}},
url = {http://www.ele.ita.br/{~}jackson/files/msc.pdf},
year = {2003}
}
@article{Monteiro2013,
author = {Monteiro, Felipe},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/c4.5vsMLP-2013 CLAGTEE{\_}submission{\_}141.pdf:pdf},
number = {October},
title = {{aplicados a Avalia{\c{c}}{\~{a}}o da Seguran{\c{c}}a Din{\^{a}}mica e}},
year = {2013}
}
@book{RusselStuart.Norvig2013,
address = {Rio de Janeiro},
author = {Russel, Stuart and Norvig, Peter},
edition = {3{\textordfeminine}},
editor = {Ltda, Elsevier Editora},
file = {:home/tarcisio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russel, Norvig - 2013 - Intelig{\^{e}}ncia Artificial.pdf:pdf},
isbn = {9780136042594},
title = {{Intelig{\^{e}}ncia Artificial}},
year = {2013}
}
@article{Kotsiantis2006,
abstract = {A discretization algorithm is needed in order to handle problems with real-valued attributes with Decision Trees (DTs), Bayesian Networks (BNs) and Rule-Learners (RLs), treating the resulting intervals as nominal val- ues. The performance of these systems is tied to the right election of these in- tervals. A good discretization algorithm has to balance the loss of information intrinsic to this kind of process and generating a reasonable number of cut points, that is, a reasonable search space. This paper presents the well known discretization techniques. Of course, a single article cannot be a complete re- view of all discretization algorithms. Despite this, we hope that the references cited cover the major theoretical issues and guide the researcher to interesting research directions and suggest possible combinations that have to be explored.},
author = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris},
doi = {10.1016/B978-044452781-3/50006-2},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/discretizationTechniques.pdf:pdf},
isbn = {9780444527813},
journal = {GESTS International Transactions on Computer Science and Engineering},
number = {1},
pages = {47--58},
title = {{Discretization Techniques : A recent survey}},
volume = {32},
year = {2006}
}
@inproceedings{Mahmoud2013,
author = {Mahmoud, Anas and {Nan Niu}},
booktitle = {2013 21st International Conference on Program Comprehension (ICPC)},
doi = {10.1109/ICPC.2013.6613844},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/Evaluating software clustering algorithms in the context of program comprehension.pdf:pdf},
isbn = {978-1-4673-3092-3},
month = {may},
pages = {162--171},
publisher = {IEEE},
title = {{Evaluating software clustering algorithms in the context of program comprehension}},
url = {http://ieeexplore.ieee.org/document/6613844/},
year = {2013}
}
@article{DeLima2015,
abstract = {This work evaluates clustering techniques and pattern classification and proposes a model that uses a combination of supervised learning algorithms and unsu- pervised learning ones with the goal of creating groups and identifying which attributes can define them (labeling) applied to social network called Scientia.Net. The tests were done using a database with around 2000 users. The proposed model shows the results applied to the database Scientia.Net},
author = {de Lima, Bruno Vicente Alves and Machado, Vinicius Ponte and Lopes, Lucas Ara{\'{u}}jo},
doi = {10.1007/s13278-015-0285-x},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Automatic labeling of social network users Scientia.Net through{\_}BrunoLima.pdf:pdf},
issn = {1869-5450},
journal = {Social Network Analysis and Mining},
keywords = {Classification,Cluster,Machine learning,Profile,Scientia.Net},
month = {dec},
number = {1},
pages = {44},
title = {{Automatic labeling of social network users Scientia.Net through the machine learning supervised application}},
url = {http://link.springer.com/10.1007/s13278-015-0285-x},
volume = {5},
year = {2015}
}
@misc{Song1990,
abstract = {The key geometric attribute of a major class of welds is the depth of penetration. However, no robust method exists for direct measurement of this quantity, although both ultrasonic and X-ray approaches have been attempted. In this paper, a method for real-time estimation of the depth is presented. The estimator is based on an inverse heat transfer solution whereby accessible surface temperatures on the top and back side of the weldment are used to solve for the isotherms internal to the weldment. The depth is then found by seeking the isotherm corresponding to the melting temperature of the material. The solution employs a three-dimensional analytical heat conduction relationship with a multiple heat source description that is shown to adequately describe many types of weld cross sections. Using a combined Gauss-Newton and steepest descent method, the measured surface temperatures are used to drive an iterative solution for the necessary heat source description (intensity and distribution). Open-loop experiments under a variety of welding conditions using four- and six-point surface radiation measurements and involving destructive measurement of the weld profile indicate that the method can provide depth estimates of acceptable accuracy for in-process control.},
author = {Song, Jae Bok and Hardt, David E.},
booktitle = {American Society of Mechanical Engineers, Dynamic Systems and Control Division (Publication) DSC},
doi = {10.1007/3-540-44673-7},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Goldberg{\_}Genetic{\_}Algorithms{\_}in{\_}Search.pdf:pdf},
isbn = {0-201-15767-5},
pages = {39--45},
title = {{Estimation of weld bead depth for in-process control}},
url = {https://koreauniv.pure.elsevier.com/en/publications/estimation-of-weld-bead-depth-for-in-process-control},
volume = {22},
year = {1990}
}
@article{VonLuxburg2008,
abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
archivePrefix = {arXiv},
arxivId = {0810.4752},
author = {von Luxburg, Ulrike and Schoelkopf, Bernhard},
doi = {10.1016/B978-0-444-52936-7.50016-1},
eprint = {0810.4752},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/statisticalLearningTheory.pdf:pdf},
issn = {18745857},
journal = {Learning},
pages = {1--40},
title = {{Statistical Learning Theory: Models, Concepts, and Results}},
url = {http://arxiv.org/abs/0810.4752},
year = {2008}
}
@book{Bishop2013,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2013}
}
@article{Srividya2018,
author = {Srividya, M. and Mohanavalli, S. and Bhalaji, N.},
doi = {10.1007/s10916-018-0934-5},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Behavioral Modeling for Mental Health using Machine Learning Algorithms.pdf:pdf},
issn = {0148-5598},
journal = {Journal of Medical Systems},
month = {may},
number = {5},
pages = {88},
title = {{Behavioral Modeling for Mental Health using Machine Learning Algorithms}},
url = {http://link.springer.com/10.1007/s10916-018-0934-5},
volume = {42},
year = {2018}
}
@article{Loh2010,
author = {Loh, Wei-Yin},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/loh{\_}slides{\_}IRIS{\_}CARS.pdf:pdf},
journal = {Technicalities},
pages = {18},
title = {{A Brief History of Classification}},
url = {http://web.ebscohost.com.pitt.idm.oclc.org/ehost/pdfviewer/pdfviewer?vid=111{\&}sid=3ba6f675-4abc-4b03-bf5b-badf431d95cc@sessionmgr14{\&}hid=26},
year = {2010}
}
@book{Barber2011,
abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Barber, David},
doi = {10.1017/CBO9780511804779},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Bayesian{\_}reasoning{\_}and{\_}Machine{\_}learning{\_}DAVID{\_}BARBER.pdf:pdf},
isbn = {9780511804779},
issn = {9780521518147},
pmid = {16931139},
title = {{Bayesian Reasoning and Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779},
year = {2011}
}
@article{Spanakis2012,
author = {Spanakis, G. and Siolas, G. and Stafylopatis, A.},
doi = {10.1093/comjnl/bxr024},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Exploiting{\_}Wikipedia{\_}Knowledge{\_}for{\_}Conceptual{\_}Hier.pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
month = {mar},
number = {3},
pages = {299--312},
title = {{Exploiting Wikipedia Knowledge for Conceptual Hierarchical Clustering of Documents}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxr024},
volume = {55},
year = {2012}
}
@article{kotsiantis2005logitboost,
abstract = {The ensembles of simple Bayesian classifiers have traditionally not been a focus of research. The reason is that simple Bayes is an extremely stable learning algorithm and most ensemb le techniques such as bagging is mainly variance reduction techniques, thus not being able to benefit from its integration. However, simple Bayes can be effectively used in ensemble techniques, which perform also bias reduction, such as Logitboost. Ho wever, Logitboost requires a regre ssion algorithm for base learner. For this reason, we slightly modify simple Bayesian cl assifier in order to be able to run as a regression method. Finally, we performed a large-scale comparison on 27 standard benchmark datasets with other state-of-the-art algorithms and ensembles using the simple Bayesian algorithm as base learner and the proposed technique was more accurate in most cases.},
address = {Patras},
author = {Kotsiantis, S B and Pintelas, P E},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/logitboostOfSimpleBayesianClassifier.pdf:pdf},
keywords = {2004,bayesian classifiers have traditionally,ensemble techniques such as,is an extremely stable,is that simple bayes,learning algorithm and most,not been a focus,november 30,of research,predictive data mining,received,supervised machine learning,the ensembles of simple,the reason},
pages = {53--59},
title = {{Logitboost of Simple Bayesian Classifier}},
volume = {29},
year = {2005}
}
@article{Hwang2002,
abstract = {A discretization technique converts continuous attribute values into discrete ones. Discretization is needed when classification algorithms require only discrete attributes. It is also useful to increase the speed and the accuracy of classification algorithms. This paper presents a dynamic discretization method, whose main characteristic is to detect interdependencies between all continuous attributes. Empirical evaluation on 12 datasets from the UCI repository shows that the proposed algorithm is a relatively effective method for discretization.},
author = {Hwang, Grace J and Li, Fumin},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dynamicMethodforDiscretizationofContinuousAttributes.pdf:pdf},
isbn = {9783540440253},
issn = {16113349},
journal = {Lecture Notes in Computer Science - Intelligent Data Engineering and Automated Learning - IDEAL 2002: Third International Conference},
pages = {506},
title = {{A Dynamic Method for Discretization of Continuous Attributes}},
url = {http://www.springerlink.com/content/4n05b2n6x0cx4tlk},
volume = {2412/2002},
year = {2002}
}
@book{Mohri2012,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Mehryar{\_}Mohri{\_}Afshin{\_}Rostamizadeh{\_}Ameet{\_}Talwalkar(BookFi.org).pdf:pdf},
isbn = {9780262018258},
title = {{Foundations Machine Learning}},
year = {2012}
}
@phdthesis{Edmar1999,
author = {Edmar, Martineli},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Tese{\_}Edmar.pdf:pdf},
pages = {1--104},
title = {{Extra{\c{c}}{\~{a}}o de conhecimento de redes neurais artificiais}},
year = {1999}
}
@article{Maqbool2006a,
author = {Maqbool, O. and Babri, H.A.},
doi = {10.1016/j.jss.2006.03.013},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Automated software clustering$\backslash$: An insight using cluster labels.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
month = {nov},
number = {11},
pages = {1632--1648},
title = {{Automated software clustering: An insight using cluster labels}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121206000768},
volume = {79},
year = {2006}
}
@incollection{Stolpe2011,
abstract = {In a supervised learning scenario, we learn a mapping from input to output values, based on labeled examples. Can we learn such a mapping also from groups of unlabeled observations, only knowing, for each group, the proportion of observations with a particular label? Solu- tions have real world applications. Here, we consider groups of steel sticks as samples in quality control. Since the steel sticks cannot be marked in- dividually, for each group of sticks it is only known how many sticks of high (low) quality it contains. We want to predict the achieved quality for each stick before it reaches the final production station and quality control, in order to save resources. We define the problem of learning from label proportions and present a solution based on clustering. Our method empirically shows a better prediction performance than recent approaches based on probabilistic SVMs, Kernel k-Means or conditional exponential models},
address = {Berlin, Heidelberg},
author = {Stolpe, Marco and Morik, Katharina},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
doi = {10.1007/978-3-642-23808-6_23},
editor = {Gunopulos, Dimitrios and and Hofmann, Thomas and and Malerba, Donato and and Vazirgiannis, Michalis},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Learning from Label Proportions by Optimizing Cluster Model Selection.pdf:pdf},
isbn = {978-3-642-23808-6},
pages = {349--364},
publisher = {Springer Berlin Heidelberg},
title = {{Learning from Label Proportions by Optimizing Cluster Model Selection}},
url = {http://link.springer.com/10.1007/978-3-642-23808-6{\_}23},
year = {2011}
}
@article{Method1992,
author = {Method, Distance-based Discretization and Cerquides, Jesus and Mantaras, Ramon Lopez De},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/1997-Cerquides-kdd.pdf:pdf},
journal = {Kdd},
pages = {139--142},
title = {{Proposal and Empirical Comparison of a Parallelizable}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.7428},
year = {1992}
}
@article{Quinlan1986,
abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
author = {Quinlan, J. R.},
doi = {10.1023/A:1022643204877},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/quinlan.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {classification,decision trees,expert systems,induction,information theory,knowledge acquisition},
number = {1},
pages = {81--106},
pmid = {17050186},
title = {{Induction of Decision Trees}},
volume = {1},
year = {1986}
}
@article{Kumar2013,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities have made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval. I.},
author = {Kumar, Ashok and Andu, Thavani and Thanamani, Antony Selvdoss},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/MultidimensionalClusteringMethods.pdf:pdf},
journal = {International Journal of Engineering Science Invention},
keywords = {cluster,data mining},
mendeley-tags = {cluster,data mining},
number = {7},
pages = {1--8},
title = {{Multidimensional Clustering Methods of Data Mining for Industrial Applications}},
volume = {2},
year = {2013}
}
@article{Chen2011a,
author = {Chen, Chun-Ling and Tseng, Frank S. C. and Liang, Tyne},
doi = {10.1007/s10115-010-0364-2},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/An integration of fuzzy association rules and WordNet.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
month = {sep},
number = {3},
pages = {687--708},
title = {{An integration of fuzzy association rules and WordNet for document clustering}},
url = {http://link.springer.com/10.1007/s10115-010-0364-2},
volume = {28},
year = {2011}
}
@inproceedings{Lucena2015,
abstract = {Classification is one of the most traditional tasks in machine learning. In supervised learning for classification, the goal is to learn a classifier function using a completely labeled dataset. Semi-supervised learning modifies the learning algorithm function allowing the use of partially labeled data. Single-label classification assigns only one label to each instance in the dataset, while multi-label classification can assign multiple labels for each instance. It would be relevant to develop techniques that are both multi-label and semi-supervised. However, few previous work has been devoted to semi-supervised multi-label classification. In the current work, we propose two new algorithms by extending the Multi-label k-Nearest Neighbors (MLkNN) algorithm to semi-supervised learning. The original MLkNN is a graph-based supervised algorithm. In our proposal, we augmented the graph structure and adapted two semi-supervised algorithms, label propagation and label spreading, for performing the label expansion in the augmented graph. We compare the proposed algorithms with a group of baseline supervised multi-label algorithms. The results for the metrics analyzed showed that the new algorithms were suitable for the multi-label semi-supervised scenarios},
author = {de Lucena, Danilo C.G. and Prudencio, Ricardo B.C.},
booktitle = {2015 Brazilian Conference on Intelligent Systems (BRACIS)},
doi = {10.1109/BRACIS.2015.26},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Semi-Supervised Multi-label k-Nearest Neighbor Classification Algorithms.pdf:pdf},
isbn = {978-1-5090-0016-6},
keywords = {label propagation,label spreading,multi-label classification,multi-label k-nearest neighbors,semi-supervised learning},
month = {nov},
pages = {49--54},
publisher = {IEEE},
title = {{Semi-supervised Multi-label k-Nearest Neighbors Classification Algorithms}},
url = {http://ieeexplore.ieee.org/document/7423994/},
year = {2015}
}
@misc{Araujo2018,
abstract = {O agrupamento (clusteriza{\c{c}}{\~{a}}o) {\'{e}} uma das principais t{\'{e}}cnicas de reconhecimento de padr{\~{o}}es. Essa t{\'{e}}cnica consiste em identificar grupos (clusters) de elementos em um determinado con- junto de dados, levando em considera{\c{c}}{\~{a}}o m{\'{e}}tricas que permitam determinar a semelhan{\c{c}}a entre eles. Os elementos presentes nesses conjuntos de dados (data sets) frequentemente s{\~{a}}o descritos por meio de atributos, os quais podem assumir valores de diversos tipos, exigindo m{\'{e}}todos eficientes na tarefa de detectar correla{\c{c}}{\~{o}}es entre dados de tipos comple- xos (ou mistos). No entanto, o processo de clusteriza{\c{c}}{\~{a}}o n{\~{a}}o fornece informa{\c{c}}{\~{o}}es claras que permitam inferir as caracter{\'{i}}sticas de cada cluster formado, ou seja, o resultado do processo de clusteriza{\c{c}}{\~{a}}o n{\~{a}}o permite que os clusters tenham seu significado facilmente compreendido. A rotula{\c{c}}{\~{a}}o de dados visa identificar essas caracter{\'{i}}sticas e permitir ent{\~{a}}o que se tenha a plena compreens{\~{a}}o dos clusters resultantes. Neste trabalho prop{\~{o}}e-se a utiliza{\c{c}}{\~{a}}o em conjunto de m{\'{e}}todos de Aprendizagem de M{\'{a}}quina n{\~{a}}o supervisionada e supervisionada para as tarefas de agrupamento e rotula{\c{c}}{\~{a}}o de dados, respectivamente. Os algoritmos DAMICORE e sua nova vers{\~{a}}o, o DAMICORE-2 (ambos reconhecidamente eficientes) foram utilizados para detectar clusters que posteriormente foram submetidos ao M{\'{e}}todo de Rotula{\c{c}}{\~{a}}o Autom{\'{a}}tica de clusters (MRA), obtendo taxas de acerto m{\'{e}}dia, entre todos os conjuntos de dados, de 86,75{\%}},
address = {Teresina - PI},
author = {Ara{\'{u}}jo, F N Carvalho},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dissertacao{\_}neto{\_}vinicius.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}} - UFPI},
keywords = {Agrupamento,Aprendizagem de M{\'{a}}quina,Rotula{\c{c}}{\~{a}}o Autom{\'{a}}tica},
pages = {48},
title = {{Rotula{\c{c}}{\~{a}}o Autom{\'{a}}tica de Clusters Baseados em An{\'{a}}lise de Filogenias}},
year = {2018}
}
@article{Gan2013,
abstract = {Semi-supervised classification has become an active topic recently and a number of algorithms, such as Self-training, have been proposed to improve the performance of supervised classification using unlabeled data. In this paper, we propose a semi-supervised learning framework which combines clustering and classification. Our motivation is that clustering analysis is a powerful knowledge-discovery tool and it may reveal the underlying data space structure from unlabeled data. In our framework, semi-supervised clustering is integrated into Self-training classification to help train a better classifier. In particular, the semi-supervised fuzzy c-means algorithm and support vector machines are used for clustering and classification, respectively. Experimental results on artificial and real datasets demonstrate the advantages of the proposed framework.},
annote = {utilizaram an{\'{a}}lise de agrupamento para prover uma classifica{\c{c}}{\~{a}}o
semissupervisionada. {\'{E}} realizado um tipo de auto-treinamento, onde algoritmo fuzzy c-means e a m{\'{a}}quina de vetor de suporte s{\~{a}}o usados para agrupamento e classifica{\c{c}}{\~{a}}o simultaneamente. Primeiramente {\'{e}} realizado o agrupamento semissupervisionado com o fuzzy c-means. O processo de agrupamento gera graus de pertin{\^{e}}ncia de cada amostra n{\~{a}}o-rotulada a diferentes classes. Os exemplos n{\~{a}}o-rotulados que obtiveram um alto grau de pertin{\^{e}}ncia a uma classe {\'{e}} ent{\~{a}}o classificado por um classificador treinado utilizando os dados rotulados. Os dados marcados e classificados com mais confian{\c{c}}as s{\~{a}}o adicionados ao conjunto de dados rotulados. Esse processo {\'{e}} repetido at{\'{e}} que os dados n{\~{a}}o-rotulados sejam totalmente rotulados.},
author = {Gan, Haitao and Sang, Nong and Huang, Rui and Tong, Xiaojun and Dan, Zhiping},
doi = {10.1016/j.neucom.2012.08.020},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Using clustering analysis to improve semi-supervised classification.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Fuzzy c-means,Self-training,Semi-supervised classification,Semi-supervised clustering,Support vector machine},
month = {feb},
pages = {290--298},
title = {{Using clustering analysis to improve semi-supervised classification}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231212006601},
volume = {101},
year = {2013}
}
@inproceedings{Lucca2013,
abstract = {Este artigo apresenta uma ferramenta para classifica{\c{c}}{\~{a}}o de texto baseada no algoritmo Na{\"{i}}ve Bayes. S{\~{a}}o descritos alguns conceitos b{\'{a}}sicos sobre classifica{\c{c}}{\~{a}}o textual na {\'{a}}rea Recupera{\c{c}}{\~{a}}o de Informa{\c{c}}{\~{o}}es, o algoritmo escolhido, um exemplo de utiliza{\c{c}}{\~{a}}o e a arquitetura da ferramenta.},
address = {Rio Grande - RS},
author = {Lucca, G and Pereira, I A and Prisco, A and Borges, E N},
booktitle = {IX Escola Regional de Banco de Dados – ERBD 2013},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/0019.pdf:pdf},
keywords = {naive bayes},
mendeley-tags = {naive bayes},
organization = {Centro de Ci{\^{e}}ncias Computacionais da Universidade Federal do Rio Grande},
pages = {1--4},
title = {{Uma implementa{\c{c}}{\~{a}}o do algoritmo Na{\"{i}}ve Bayes para classifica{\c{c}}{\~{a}}o de texto}},
url = {http://ifc-camboriu.edu.br/erbd2013},
year = {2013}
}
@article{Escalante2011,
author = {Escalante, Hugo Jair and Montes-y-Gom{\'{e}}z, Manuel and Sucar, Luis Enrique},
doi = {10.1016/j.cviu.2011.02.002},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/An energy-based model for region-labeling.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = {jun},
number = {6},
pages = {787--803},
title = {{An energy-based model for region-labeling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314211000397},
volume = {115},
year = {2011}
}
@techreport{Aeberhard1992,
address = {North Queensland, Austr{\'{a}}lia},
author = {Aeberhard, S. and Coomans, D. and Vel, O. D.},
institution = {Department of Mathematics and Statistics, James Cook University},
pages = {92--02},
title = {{Comparison of classifiers in high dimensional settings.}},
year = {1992}
}
@book{Montgomery2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Montgomery, Karen},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
edition = {1},
editor = {{O'Reilly Media}, Inc.},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/big-data-now-2012.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
publisher = {O'Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472},
title = {{Big Data Now}},
volume = {53},
year = {2013}
}
@inproceedings{Pq,
abstract = {When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.},
address = {Montr{\'{e}}al, Qu{\'{e}}, Canada},
author = {George, H. John and Langley, Pat},
booktitle = {UAI'95 Proceedings of the Eleventh conference on Uncertainty in artificial intelligence},
editor = {Besnard, Philippe (IRISA Rennes / France) and Hanks, Steve (University of Washington / Seattle)},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/artigo{\_}bayes-continuous.pdf:pdf},
keywords = {bayes},
mendeley-tags = {bayes},
pages = {338--345},
publisher = {Morgan Kalfmann Publishers},
title = {{Estimating Continuous Distributions in Bayesian Classifiers}},
url = {http://dl.acm.org/citation.cfm?id=2074158.2074196},
year = {1995}
}
@article{Prati2006,
abstract = {Machine learning algorithms are often the most appropriate algorithms for a great variety of data mining applications. However, most machine lear- ning research to date has mainly dealt with the well-circumscribed problem of finding a model (generally a classifier) given a single, small and relatively clean dataset in the attribute-value form, where the attributes have previ- ously been chosen to facilitate learning. Furthermore, the end-goal is simple and well-defined, such as accurate classifiers in the classification problem. Data mining opens up new directions for machine learning research, and lends new urgency to others. With data mining, machine learning is now re- moving each one of these constraints. Therefore, machine learning's many valuable contributions to data mining are reciprocated by the latter's invi- gorating effect on it. In this thesis, we explore this interaction by proposing new solutions to some problems due to the application of machine learning algorithms to data mining applications. More specifically, we contribute to the following problems. New approaches to rule learning. In this category, we propose two new methods for rule learning. In the first one, we propose a new method for finding exceptions to general rules. The second one is a rule selection algorithm based on the ROC graph. Rules come from an external larger set of rules and the algorithm performs a selection step based on the current convex hull in the ROC graph. Proportion of examples among classes. We investigated several as- pects related to this issue. Firstly, we carried out a series of experiments on artificial data sets in order to verify our hypothesis that overlapping among classes is a complicating factor in highly skewed data sets. We also carried out a broadly experimental analysis with several methods (some of them proposed by us) that artificially balance skewed datasets. Our experiments show that, in general, over-sampling methods perform better than under- sampling methods. Finally, we investigated the relationship between class imbalance and small disjuncts, as well as the influence of the proportion of examples among classes in the process of labelling unlabelled cases in the semi-supervised learning algorithm Co-training. New method for combining rankings. We propose a new method cal- led BORDARANK to construct ensembles of rankings based on borda count voting, which could be applied whenever only the rankings are available. Results show an improvement upon the base-rankings constructed by ta- king into account the ordering given by classifiers which output continuous- valued scores, as well as a comparable performance with the fusion of such scores.},
author = {Prati, Ronaldo Cristiano},
doi = {10.11606/T.55.2006.tde-01092006-155445},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/RonaldoPratiTese.pdf:pdf},
pages = {191},
title = {{Novas abordagens em aprendizado de m{\'{a}}quina para a gera{\c{c}}{\~{a}}o de regras, classes desbalanceadas e ordena{\c{c}}{\~{a}}o de casos - Tese de Doutorado}},
year = {2006}
}
@article{Dougherty1995,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
address = {Stanford},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
doi = {10.1016/B978-1-55860-377-6.50032-3},
eprint = {9809069v1},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/supervisedUnsupervisedDiscretization.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Machine Learning Proceedings 1995},
pages = {194--202},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Supervised and Unsupervised Discretization of Continuous Features}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603776500323},
volume = {0},
year = {1995}
}
@article{Lorenzett2016,
author = {Lorenzett, Cassio dal Castel and Teloken, Alex},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/4023-10934-1-PB.pdf:pdf},
journal = {Simp{\'{o}}sio de Pesquisa e Desenvolvimento em Computa{\c{c}}{\~{a}}o},
number = {1},
title = {{Estudo Comparativo entre os algoritmos de Minera{\c{c}}{\~{a}}o de Dados Random Forest e J48 na tomada de Decis{\~{a}}o}},
volume = {2},
year = {2016}
}
@misc{Federal2016,
author = {Federal, Universidade and Piau{\'{i}}, D O and Propesq, Pr{\'{o}}-reitoria D E Pesquisa},
file = {:home/tarcisio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Federal, Piau{\'{i}}, Propesq - 2016 - Processo de Descoberta de Conhecimento em Base de Dados para previs{\~{a}}o de ocorr{\^{e}}ncias de Esp{\'{e}}cimes d.pdf:pdf},
title = {{Processo de Descoberta de Conhecimento em Base de Dados para previs{\~{a}}o de ocorr{\^{e}}ncias de Esp{\'{e}}cimes de Peixe Boi Marinho}},
year = {2016}
}
@book{Pyle1999,
abstract = {Data preparation is a fundamental stage of data analysis. While a lot of low-quality information is available in various data sources and on the Web, many organizations or companies are interested in how to transform the data into cleaned forms which can be used for high-profit purposes. This goal generates an urgent need for data analysis aimed at cleaning the raw data. In this paper, we first show the importance of data preparation in data analysis, then introduce some research achievements in the area of data preparation. Finally, we suggest some future directions of research and development. Data preparation is a fundamental stage of data analysis. While a lot of low-quality information is available in various data sources and on the Web, many organizations or companies are interested in how to transform the data into cleaned forms which can be used for high-profit purposes. This goal generates an urgent need for data analysis aimed at cleaning the raw data. In this paper, we first show the importance of data preparation in data analysis, then introduce some research achievements in the area of data preparation. Finally, we suggest some future directions of research and development.},
author = {Pyle, Dorian and Editor, Senior and Cerra, Diane D},
booktitle = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
doi = {10.1080/713827180},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/DataPreparationForDataMining-DorianPyle.pdf:pdf},
isbn = {4159822665},
issn = {08839514},
keywords = {counter-terrorism,data mining,privacy},
pages = {375--381},
pmid = {4047876},
title = {{Data Preparation for Data Mining}},
url = {http://www.tandfonline.com/doi/abs/10.1080/713827180},
volume = {17},
year = {1999}
}
@article{ENS2001,
abstract = {O presente artigo relata a proposta na PUCPR de unir pesquisa e doc{\^{e}}ncia na forma{\c{c}}{\~{a}}o continuada de professores, mais especificamente dos estudos sobre a abordagem qualitativa e a metodologia da pesquisa que est{\'{a}} sendo utilizada no projeto Gest{\~{a}}o Estrat{\'{e}}gica de Compet{\^{e}}ncias e a Forma{\c{c}}{\~{a}}o do Professor. A realiza{\c{c}}{\~{a}}o da pesquisa vem propiciando ao grupo de professores da {\'{a}}rea de educa{\c{c}}{\~{a}}o um trabalho integrado, e ao mesmo tempo, um caminhar pela pes- quisa-a{\c{c}}{\~{a}}o, integrando os seguintes procedimentos e t{\'{e}}cnicas de pesquisa: an{\'{a}}- lise documental, an{\'{a}}lise iconogr{\'{a}}fica, aplica{\c{c}}{\~{a}}o de question{\'{a}}rios, entrevista semi-estruturada, observa{\c{c}}{\~{a}}o participante e semin{\'{a}}rios},
author = {ENS, Romilda Teodora and Ploharski, Nara Regina and SALLES, Suely Therezinha Costa},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dialogo-740.pdf:pdf},
journal = {Revista Di{\'{a}}logo Educacional},
keywords = {metodologia da pesquisa,pesquisa,pesquisa-a{\c{c}}{\~{a}}o,procedi- mentos,t{\'{e}}cnicas de pesquisa},
number = {4},
pages = {67--84},
title = {{a Pesquisa E O Fazer Pedag{\'{o}}gico : Gerar E DIFUNDIR CONHECIMENTOS}},
volume = {2},
year = {2001}
}
@phdthesis{Madureira2017,
abstract = {Um grande n´ umero de mensagens curtas informais s˜ ao postadas dia- riamente em redes sociais, f´ orums de discuss˜ ao e pesquisas de satisfa¸ao. c˜ Emo¸oes parecem ser importantes de forma frequente nesses textos. O de- c˜ safio de identificar e entender a emo¸ao presente nesse tipo de comunica¸aoc˜ c˜ ´ e importante para distinguir o sentimento presente no texto e tamb´ em para identificar comportamentos anˆ omalos e inapropriados, eventualmente ofere- cendo algum tipo de risco. Este trabalho prop˜ oe a implementa¸ao de uma solu¸ao para a an´ c˜ c˜ alise de sentimento de textos curtos baseada em aprendizado por m´ aquina. Utili- zando t´ ecnicas de aprendizado supervisionado, ´ e desejado discernir se uma mensagem possui sentimento positivo, neutro ou negativo. As mensagens a serem analisadas ser˜ ao pesquisas de satisfa¸ao de servi¸ c˜ cos de TI. Foram uti- lizados nas an´ alises dois modelos, o primeiro modelo onde apenas o campo de texto livre ”Coment´ ario”foi considerado e o segundo modelo, onde al´ em do campo de texto livre ”Coment´ ario”, foram consideradas, adicionalmente, duas perguntas objetivas da pesquisa de satisfa¸ao. c˜ Os resultados obtidos indicam que as t´ ecnicas utilizadas de aprendizado por m´ aquina, n˜ ao ficam atr´ as dos resultados produzidos por aprendizado humano. A acur´ acia obtida foi de at´ e 86,8{\%}de acerto para ummodelo de trˆ es classes: ”elogio”, ”neutro”e ”reclama¸ao”. A acur´ c˜ acia foi significativamente superior, alcan¸ cando at´ e 94,5{\%} em um modelo alternativo, de apenas duas classes: ”elogio”e ”n˜ ao-elogio”},
address = {Rio de Janeiro},
author = {Madureira, Daniel Fialho},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/FGV EMAp - Gustavo Avila - An{\'{a}}lise de Sentimento para Textos Curtos.pdf:pdf},
school = {Fundacao Getulio Vargas},
title = {{Analise de sentimento para textos curtos}},
year = {2017}
}
@inproceedings{Treeratpituk2006,
address = {New York, New York, USA},
author = {Treeratpituk, Pucktada and Callan, Jamie},
booktitle = {Proceedings of the 2006 national conference on Digital government research - dg.o '06},
doi = {10.1145/1146598.1146650},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/Automatically{\_}labeling{\_} hierarchical{\_} clusters.pdf:pdf},
pages = {167},
publisher = {ACM Press},
title = {{Automatically labeling hierarchical clusters}},
url = {http://portal.acm.org/citation.cfm?doid=1146598.1146650},
year = {2006}
}
@book{Bishop2013,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2013}
}
@inproceedings{Iwamura2013,
author = {Iwamura, Masakazu and Tsukada, Masaki and Kise, Koichi},
booktitle = {2013 12th International Conference on Document Analysis and Recognition},
doi = {10.1109/ICDAR.2013.276},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Automatic Labeling for Scene Text Database.pdf:pdf},
isbn = {978-0-7695-4999-6},
month = {aug},
pages = {1365--1369},
publisher = {IEEE},
title = {{Automatic Labeling for Scene Text Database}},
url = {http://ieeexplore.ieee.org/document/6628837/},
year = {2013}
}
@article{Mccallum1997,
author = {Mccallum, Andrew and Nigam, Kamal},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/multinomial-aaaiws98.pdf:pdf},
title = {{A Comparison of Event Models for Naive Bayes Text Classification}},
year = {1997}
}
@article{Maqbool2006,
author = {Maqbool, O. and Babri, H.A.},
doi = {10.1016/j.jss.2006.03.013},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Automated software clustering$\backslash$: An insight using cluster labels.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
month = {nov},
number = {11},
pages = {1632--1648},
title = {{Automated software clustering: An insight using cluster labels}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121206000768},
volume = {79},
year = {2006}
}
@book{Mitchell1997,
abstract = {Um texto introdut{\'{o}}rio sobre abordagens prim{\'{a}}rias para a aprendizagem de m{\'{a}}quinas e o estudo de algoritmos de computador que melhoram automaticamente atrav{\'{e}}s da experi{\^{e}}ncia. Introduza conceitos b{\'{a}}sicos de estat{\'{i}}stica, intelig{\^{e}}ncia artificial, teoria da informa{\c{c}}{\~{a}}o e outras disciplinas, conforme necess{\'{a}}rio, com cobertura equilibrada de teoria e pr{\'{a}}tica e apresenta algoritmos importantes com ilustra{\c{c}}{\~{o}}es de seu uso. Inclui exerc{\'{i}}cios de cap{\'{i}}tulo. Conjuntos de dados on-line e implementa{\c{c}}{\~{o}}es de v{\'{a}}rios algoritmos est{\~{a}}o dispon{\'{i}}veis em um site. Nenhum conhecimento pr{\'{e}}vio em intelig{\^{e}}ncia artificial ou estat{\'{i}}stica {\'{e}} assumido. Para graduados avan{\c{c}}ados e estudantes de p{\'{o}}s-gradua{\c{c}}{\~{a}}o em inform{\'{a}}tica, engenharia, estat{\'{i}}stica e ci{\^{e}}ncias sociais, bem como profissionais de software.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Mitchell, Tom M.},
booktitle = {McGraw-Hill Science/Engineering/Math},
doi = {10.1007/978-3-540-75488-6_2},
eprint = {0-387-31073-8},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/McGrawHill{\_}-{\_}Machine{\_}Learning{\_}-Tom{\_}Mitchell.pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
pages = {432},
pmid = {18292226},
publisher = {McGraw-Hill Science/Engineering/Math},
title = {{Machine learning}},
year = {1997}
}
@inproceedings{Jirasirilerd2018,
author = {Jirasirilerd, Wiphada and Tangtisanon, Pikulkaew},
booktitle = {2018 International Conference on Engineering, Applied Sciences, and Technology (ICEAST)},
doi = {10.1109/ICEAST.2018.8434457},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Automatic labeling for Thai News Articles Based on Vector Representation of Documents.pdf:pdf},
isbn = {978-1-5386-4956-5},
month = {jul},
pages = {1--4},
publisher = {IEEE},
title = {{Automatic Labeling for Thai News Articles Based on Vector Representation of Documents}},
url = {https://ieeexplore.ieee.org/document/8434457/},
year = {2018}
}
@article{Iria2009,
abstract = {We present an approach to automating knowledge extraction in the aerospace engineering domain which has had a fundamental impact on the way engineers manage their collective knowledge built with years of experience. Even though obtaining labelled data in this domain is hard due to the high cost of domain experts' time, the application of the machine learning-based technology was successful, yielding results comparable to the state-of-the-art. Moreover, we present a comparison between several machine learning approaches in extracting knowledge from reports about jet engines. We show that the application of a semi-supervised approach does not provide a significant increase in accuracy so as to justify its adoption due to its much higher computational cost, but that the application of a large-scale approach considerably reduces both training and testing time while keeping accuracy comparable to the standard supervised approach, making it a good choice for this class of application scenarios.},
author = {Iria, Jos{\'{e}}},
doi = {10.1145/1597735.1597753},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p97-iria.pdf:pdf},
isbn = {9781605586588},
journal = {Proceedings of the fifth international conference on Knowledge capture - K-CAP '09},
keywords = {aerospace,information extraction,knowledge capture,machine learning},
pages = {97--104},
title = {{Automating knowledge capture in the aerospace domain}},
url = {http://dl.acm.org/citation.cfm?id=1597735.1597753},
year = {2009}
}
@misc{LOPES2014DISSERT,
abstract = {O problema de agrupamento (clustering) tem sido considerado como um dos problemas mais relevantes dentre aqueles existentes na {\'{a}}rea de pesquisa de aprendizagem n{\~{a}}o- supervisionada (sub{\'{a}}rea de Aprendizagem de M{\'{a}}quina). Embora o desenvolvimento e aprimoramento de algoritmos que solucionam esse problema tenha sido o principal foco de muitos pesquisadores o objetivo inicial se manteve obscuro: a compreens{\~{a}}o dos grupos formados. T{\~{a}}o importante quanto a identifica{\c{c}}{\~{a}}o dos grupos (clusters) {\'{e}} sua compreens{\~{a}}o e defini{\c{c}}{\~{a}}o. Uma boa defini{\c{c}}{\~{a}}o de um cluster representa um entendimento significativo e pode ajudar o especialista ao estudar ou interpretar dados. Frente ao problema de compreender clusters – isto {\'{e}}, de encontrar uma defini{\c{c}}{\~{a}}o ou em outras palavras, um r{\'{o}}tulo – este trabalho apresenta uma defini{\c{c}}{\~{a}}o para esse problema, deno- minado problema de rotula{\c{c}}{\~{a}}o, al{\'{e}}m de uma solu{\c{c}}{\~{a}}o baseada em t{\'{e}}cnicas com aprendi- zagem supervisionada, n{\~{a}}o-supervisionada e um modelo de discretiza{\c{c}}{\~{a}}o. Dessa forma, o problema {\'{e}} tratado desde sua concep{\c{c}}{\~{a}}o: o agrupamento de dados. Para isso, um m{\'{e}}todo com aprendizagem n{\~{a}}o-supervisionada {\'{e}} aplicado ao problema de clustering e ent{\~{a}}o um algoritmo com aprendizagem supervisionada ir{\'{a}} detectar quais atributos s{\~{a}}o relevantes para definir um dado cluster. Adicionalmente, algumas estrat{\'{e}}gias s{\~{a}}o utilizadas para formar uma metodologia que apresenta em sua totalidade um r{\'{o}}tulo (baseado em atributos e valores) para cada grupo fornecido. Finalmente, essa metodo- logia {\'{e}} aplicada em quatro bases de dados distintas apresentando bons resultados com uma m{\'{e}}dia acima de 93.5{\%} dos elementos rotulados corretamente..},
address = {Teresina},
author = {Lopes, Lucas A.},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dissertacao{\_}lopes.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}} - UFPI},
keywords = {agrupamento,aprendizado de m{\'{a}}quina,rotula{\c{c}}{\~{a}}o},
mendeley-tags = {agrupamento,aprendizado de m{\'{a}}quina,rotula{\c{c}}{\~{a}}o},
pages = {73},
title = {{Rotula{\c{c}}{\~{a}}o Autom{\'{a}}tica de Grupos com Aprendizagem de M{\'{a}}quina Supervisionada}},
type = {Disserta{\c{c}}{\~{a}}o (Programa de P{\'{o}}s-gradua{\c{c}}{\~{a}}o em Ci{\^{e}}ncia da Computa{\c{c}}{\~{a}}o)},
year = {2014}
}
@article{Steiner2007,
abstract = {A avalia{\c{c}}{\~{a}}o de risco de cr{\'{e}}dito {\'{e}} um importante problema administrativo da {\'{a}}rea de an{\'{a}}lise financeira. As Redes Neurais t{\^{e}}m recebido muita aten{\c{c}}{\~{a}}o pela sua alta taxa de acur{\'{a}}cia preditiva, no entanto n{\~{a}}o {\'{e}} f{\'{a}}cil compreender como elas alcan{\c{c}}am as suas decis{\~{o}}es. Neste artigo um conjunto de dados de cr{\'{e}}dito {\'{e}} analisado usando a t{\'{e}}cnica de extra{\c{c}}{\~{a}}o de regras NeuroRule e o software WEKA para a extra{\c{c}}{\~{a}}o de regras a partir de uma Rede Neural treinada. Os resultados foram considerados bastante satisfat{\'{o}}rios alcan{\c{c}}ando mais de 80{\%} de acur{\'{a}}cia quanto {\`{a}} concess{\~{a}}o (ou n{\~{a}}o) de cr{\'{e}}dito banc{\'{a}}rio em todas as simula{\c{c}}{\~{o}}es.},
author = {Steiner, Maria Teresinha Arns and Nievola, J{\'{u}}lio Cesar and Soma, Nei Yoshihiro and Shimizu, Tamio and {Steiner Neto}, Pedro Jos{\'{e}}},
doi = {10.1590/S0101-74382007000300002},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/v27n3a02{\_}regras{\_}rNA{\_}tomada{\_}decisao{\_}creditoBancario.pdf:pdf},
issn = {0101-7438},
journal = {Pesquisa Operacional},
pages = {407--426},
title = {{Extra{\c{c}}{\~{a}}o de regras de classifica{\c{c}}{\~{a}}o a partir de redes neurais para aux{\'{i}}lio {\`{a}} tomada de decis{\~{a}}o na concess{\~{a}}o de cr{\'{e}}dito banc{\'{a}}rio}},
url = {http://www.scielo.br/scielo.php?pid=S0101-74382007000300002{\&}script=sci{\_}arttext{\&}tlng=pt},
volume = {27},
year = {2007}
}
@book{Casari2018,
address = {Sebastopol, CA},
author = {Casari, Amanda and Zheng, Alice},
edition = {First Edit},
editor = {RoumeliotisRachel and Bleiel, Jeff},
isbn = {9781491953235},
publisher = {O'Reilly Media, Inc.},
title = {{Feature Engineering for Machine Learning. Principles and Techniques for Data Scientists.}},
year = {2018}
}
@unpublished{Imperes2018a,
address = {Teresina - PI},
author = {{IMPERES FILHO}, Francisco das Chagas},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/dissertacaoV7{\_}FranciscoImperes.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}} - UFPI},
keywords = {Agrupamento de Dados,Aprendizagem de M{\'{a}}quina,Defini{\c{c}}{\~{a}}o de Dados,Rotula{\c{c}}{\~{a}}o de Dados},
pages = {90},
title = {{Disserta{\c{c}}{\~{a}}o (Programa de P{\'{o}}s Gradua{\c{c}}{\~{a}}o da Ci{\^{e}}ncias da Computa{\c{c}}{\~{a}}o - PPGCC). Rotula{\c{c}}{\~{a}}o de Grupos em Algoritmos de Agrupamento Baseados em Dist{\^{a}}ncia Utilizando Grau de Pertin{\^{e}}ncia. Teresina-PI}},
year = {2018}
}
@inproceedings{Suadaa2016,
address = {Bandung, Indonesia},
author = {Suadaa, Lya Hulliyyatus and Purwarianti, Ayu},
booktitle = {2016 4th International Conference on Information and Communication Technology (ICoICT)},
doi = {10.1109/ICoICT.2016.7571885},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Combination of Latent Dirichlet Allocatio.pdf:pdf},
isbn = {9781467398794},
keywords = {cluster labeling,latent dirichlet,text clustering},
number = {c},
pages = {1--6},
publisher = {IEEE},
title = {{Combination of Latent Dirichlet Allocation ( LDA ) and Term Frequency-Inverse Cluster Frequency ( TFxICF ) in Indonesian Text Clustering with Labeling}},
volume = {4},
year = {2016}
}
@article{DeLucia2014,
author = {{De Lucia}, Andrea and {Di Penta}, Massimiliano and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
doi = {10.1007/s10664-013-9285-5},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Labeling source code with information retrieval.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {oct},
number = {5},
pages = {1383--1420},
title = {{Labeling source code with information retrieval methods: an empirical study}},
url = {http://link.springer.com/10.1007/s10664-013-9285-5},
volume = {19},
year = {2014}
}
@inproceedings{Raimundo2008,
abstract = {O avan{\c{c}}o computacional no que se refere ao processamento earmazenamento contribuiu para a forma{\c{c}}{\~{a}}o de grandes reposit{\'{o}}rios de dados,tornando-se necess{\'{a}}rio o desenvolvimento de tecnologias destinadas {\`{a}} an{\'{a}}lisede informa{\c{c}}{\~{o}}es e obten{\c{c}}{\~{a}}o de novos conhecimentos. Dentre essas tecnologiaso data mining constitui-se como uma das alternativas, utilizando para issoalgoritmos. Este artigo apresenta a modelagem matem{\'{a}}tica e implementa{\c{c}}{\~{a}}odo algoritmo CART para indu{\c{c}}{\~{a}}o de {\'{a}}rvores de decis{\~{a}}o na tarefa declassifica{\c{c}}{\~{a}}o do processo de data mining em uma Shell denominada de Orion.},
address = {Pelotas - RS},
author = {Raimundo, Lidiane Rosso and de Mattos, Merisandra C{\^{o}}rtes and Sim{\~{o}}es, Priscyla Waleska Targino de Azevedo and Cechinel, Cristian},
booktitle = {IV Congresso Sul Brasileiro de Ci{\^{e}}ncias da Computa{\c{c}}{\~{a}}o - SULCOMP},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/1994-6024-1-PB.pdf:pdf},
keywords = {CART,CART Algorithm,Classification,Data Mining,Decision Trees,data min,decision trees},
mendeley-tags = {CART,data min,decision trees},
title = {{O Algoritmo de Classifica{\c{c}}{\~{a}}o CART em uma Ferramenta de Data Mining}},
year = {2008}
}
@article{Chen2011b,
author = {Chen, Chun-Ling and Tseng, Frank S. C. and Liang, Tyne},
doi = {10.1007/s10115-010-0364-2},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/An integration of fuzzy association rules and WordNet.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
month = {sep},
number = {3},
pages = {687--708},
title = {{An integration of fuzzy association rules and WordNet for document clustering}},
url = {http://link.springer.com/10.1007/s10115-010-0364-2},
volume = {28},
year = {2011}
}
@article{Frank1999,
abstract = {Before applying learning algorithms to datasets, practitioners often globally discretize any numeric attributes. If the algorithm cannot handle numeric attributes directly, prior discretization is essential. Even if it can, prior discretization often accelerates induction, and may produce simpler and more accurate classifiers. As it is generally done, global discretization denies the learning algorithm any chance of taking advantage of the ordering information implicit in numeric attributes. However, a simple transformation of discretized data preserves this information in a form that learners can use. We show that, compared to using the discretized data directly, this transformation significantly increases the accuracy of decision trees built by C4.5, decision lists built by PART, and decision tables built using the wrapper method, on several bench-mark datasets. Moreover, it can significantly reduce the size of the resulting classifiers. This simple technique makes global discretization an even more useful tool for data preprocessing},
author = {Frank, Eibe and Witten, Ian H.},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/99EF-IHW-Globe-Discre.pdf:pdf},
keywords = {computer science,discretization},
pages = {1--12},
title = {{Making better use of global discretization}},
url = {http://researchcommons.waikato.ac.nz/handle/10289/1507},
year = {1999}
}
@article{Bruce2001,
author = {Bruce, RF},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/10.1.1.58.986.pdf:pdf},
journal = {Nlprs},
keywords = {Bayesian,semi-class},
title = {{A Bayesian Approach to Semi-Supervised Learning}},
url = {http://www.researchgate.net/publication/2930623{\_}A{\_}Bayesian{\_}Approach{\_}to{\_}Semi-Supervised{\_}Learning/file/9c96051b000243458e.pdf},
year = {2001}
}
@inproceedings{Majid2016,
abstract = {Tree species identification is a crucial matter in forest managing and supervision. In recent years, tree species identification has been studying a lot using high-resolution satellite imagery data of 0.5m spatial resolution Worldview-3. This has because of some of the tree species have very high market value type of wood. Besides that, Tropical forest stored a large stock of carbon that contributes to the huge amount of above and below ground biomass. Hence, tree species identification is important in order to manage and control such activities like critical timber logged, forest cut for agricultural and development purposes that would affect the carbon cycle and lead to global warming affect. Tree classification using conventional method is required more time and limited accessibility for site survey, so the remote sensing technology was applied to identify tree species. In this study, tree species classification was done to identify dominant tree species had in Ayer Hitam Forest Reserve Puchong to estimate biomass values. Support Vector Machine (SVM) supervised classification method was applied in this process. This study can give some ideas to researchers in the future on what type of improvements can be made for next research and is hoped to assist the forest managers to control and manage forest ecosystems in order to reduce climate change.},
author = {Majid, Ibtisam Ab and Latif, Zulkiflee Abd and Adnan, Nor Aizam},
booktitle = {2016 7th IEEE Control and System Graduate Research Colloquium (ICSGRC)},
doi = {10.1109/ICSGRC.2016.7813304},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Tree Species Classification Using Worldview-3 data.pdf:pdf},
isbn = {978-1-5090-1175-9},
keywords = {Remote Sensing,Support Vector Machine,Tree Species Classification},
month = {aug},
pages = {73--76},
publisher = {IEEE},
title = {{Tree species classification using worldview-3 data}},
url = {http://ieeexplore.ieee.org/document/7813304/},
year = {2016}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Algumas{\_}informacoes{\_}uteis{\_}sobre{\_}MachineLearning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Chen2011,
author = {Chen, Chun-Ling and Tseng, Frank S. C. and Liang, Tyne},
doi = {10.1007/s10115-010-0364-2},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/An integration of fuzzy association rules and WordNet.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
month = {sep},
number = {3},
pages = {687--708},
title = {{An integration of fuzzy association rules and WordNet for document clustering}},
url = {http://link.springer.com/10.1007/s10115-010-0364-2},
volume = {28},
year = {2011}
}
@article{Bair2013,
author = {Bair, Eric},
doi = {10.1002/wics.1270},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Semi‐supervised clustering methods.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
month = {sep},
number = {5},
pages = {349--361},
title = {{Semi-supervised clustering methods}},
url = {http://doi.wiley.com/10.1002/wics.1270},
volume = {5},
year = {2013}
}
@article{Trandafili2012,
abstract = {Higher education institutions are overwhelmed with huge amounts of information regarding student's enrollment, number of courses completed, achievement in each course, performance indicators and other data. This has led to an increasingly complex analysis process of the growing volume of data and to the incapability to take decisions regarding curricula reform and restructuring. On the other side, educational data mining is a growing field aiming at discovering knowledge from student's data in order to thoroughly understand the learning process and take appropriate actions to improve the student's performance and the quality of the courses delivery. This paper presents a thorough analysis process performed on student's data through machine learning techniques. Experiments performed on a very large real-world dataset of students performance on all courses of a university, reveal interesting and important students profiles with clustering and surprising relationships among the courses performance with association},
author = {Trandafili, Evis and Allko{\c{c}}i, Alban and Kajo, Elinda and Xhuvani, Aleksand{\"{e}}r},
doi = {10.1145/2371316.2371350},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p174-trandafili.pdf:pdf},
isbn = {9781450312400},
journal = {Proceedings of the Fifth Balkan Conference in Informatics on - BCI '12},
pages = {174},
title = {{Discovery and evaluation of student's profiles with machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2371316.2371350},
year = {2012}
}
@inproceedings{Filho2015,
abstract = {O agrupamento (clustering) de dados tem sido considerado como um dos tópicos mais rele- vantes dentre aqueles existentes na área de aprendizagem de máquina não-supervisionada. Embora o desenvolvimento e aprimoramento de algoritmos que tratam esse problema tenham sido o principal foco de muitos pesquisadores, a compreensão da definição dos grupos (clusters) é tão importante quanto sua formação. Uma boa definição de um grupo pode ajudar na interpretação dos dados. Frente ao problema de compreender a definição dos grupos este trabalho descreve uma solução que utiliza a teoria de conjuntos fuzzy para identificar os elementos mais relevantes do agrupamento e modelar faixas de valores que sejam capazes de identificar cada um dos grupos, baseando-se em caracterı́sticas únicas. Os experimentos realizados demostram que o modelo proposto é bastante factı́vel e capaz de construir faixas de valores para a identificação dos grupos, assim como classificar novos elementos utilizando as definições fornecidas.},
address = {Natal, RN},
author = {Filho, Vilmar Pereira Ribeiro and Machado, Vin{\'{i}}cius Ponte and Lira, Ricardo de Andrade},
booktitle = {XII Simp{\'{o}}sio Brasileiro de Automa{\c{c}}{\~{a}}o Inteligente (SBAI)},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/SBAI{\_}2015{\_}submission{\_}108.pdf:pdf},
keywords = {Cluster,Fuzzy,Labeling,Learning (artificial intelligence),fuzzy,rotula{\c{c}}{\~{a}}o},
mendeley-tags = {fuzzy,rotula{\c{c}}{\~{a}}o},
month = {nov},
organization = {Universidade Federal do Piau{\'{i}}},
title = {{Rotula{\c{c}}{\~{a}}o de Grupos Utilizando Conjuntos Fuzzy}},
url = {http://pubs.acs.org/doi/abs/10.1021/ja103937v},
year = {2015}
}
@article{Tatibana,
author = {Tatibana, Cassia Yuri and Kaetsu, Deisi Yuki},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/classificacao{\_}automatica{\_}artigos{\_}utilizando{\_}redes{\_}neurais.pdf:pdf},
title = {{Redes Neurais}},
url = {http://www.din.uem.br/ia/neurais/{\#}bibliografia}
}
@article{Hwang2002a,
abstract = {A discretization technique converts continuous attribute values into discrete ones. Discretization is needed when classification algorithms require only discrete attributes. It is also useful to increase the speed and the accuracy of classification algorithms. This paper presents a dynamic discretization method, whose main characteristic is to detect interdependencies between all continuous attributes. Empirical evaluation on 12 datasets from the UCI repository shows that the proposed algorithm is a relatively effective method for discretization.},
author = {Hwang, Grace J and Li, Fumin},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Hwang2002.pdf:pdf},
isbn = {9783540440253},
issn = {16113349},
journal = {Lecture Notes in Computer Science - Intelligent Data Engineering and Automated Learning - IDEAL 2002: Third International Conference},
pages = {506},
title = {{A Dynamic Method for Discretization of Continuous Attributes}},
url = {http://www.springerlink.com/content/4n05b2n6x0cx4tlk},
volume = {2412/2002},
year = {2002}
}
@article{Yohannes1999,
author = {Yohannes, Yisehac and Hoddinott, John},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Pnach725{\_}CART.pdf:pdf},
pages = {1--29},
title = {{Classification and regression trees: an introduction}},
year = {1999}
}
@inproceedings{Yang2002,
address = {Gold Coast, Austr{\'{a}}lia},
author = {Yang, Ying and Webb, Geoffrey I.},
booktitle = {Proceedings of PKAW 2002: The 2002 Pacific Rim Knowledge Acquisition Workshop.},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/A Comparative Study of Discretization Methods for Naive-Bayes Classifiers.pdf:pdf},
pages = {159--173},
title = {{A Comparative Study of Discretization Methods for Naive-Bayes Classifiers}},
year = {2002}
}
@article{Chen2010,
author = {Chen, Shyi-ming and Lee, Li-wei},
doi = {10.1109/ICSMC.2010.5642192},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/new{\_}method{\_}fuzzy{\_}group{\_}decision-marking{\_}based{\_}on{\_}interval{\_}liguistic{\_}labels.pdf:pdf},
isbn = {9781424465880},
journal = {2010 IEEE International Conference on Systems, Man and Cybernetics},
number = {5},
pages = {1--4},
publisher = {IEEE},
title = {{A new method for fuzzy group decision-making based on interval linguistic labels}}
}
@article{Charytanowicz2010,
abstract = {Methods based on kernel density estimation have been successfully applied for various data mining techniques. Their natural interpretation together with consistency properties make them an attractive tool in clustering problems. In this paper, the complete gradient clustering algorithm, based on the density of the data, is presented. The proposed method has been applied to a real data set of grains and compared with K-means clustering algorithm. The wheat varieties, Kama, Rosa and Canadian, characterized by measurements of main grain geometric features obtained by X-ray technique, have been analyzed. Results indicate that the proposed method is expected to be an effective method for recognizing wheat varieties. Moreover, it outperforms the K-means analysis if the nature of the grouping structures among the data is unknown before processing.},
author = {Charytanowicz, Ma{\l}gorzata and Niewczas, Jerzy and Kulczycki, Piotr and Kowalski, Piotr A. and {\L}ukasik, Szymon and Zak, S{\l}awomir},
doi = {10.1007/978-3-642-13105-9_2},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Charytanowicz{\_}et{\_}al A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images.pdf:pdf},
isbn = {9783642131042},
issn = {18675662},
journal = {Advances in Intelligent and Soft Computing},
pages = {15--24},
title = {{Complete gradient clustering algorithm for features analysis of X-ray images}},
volume = {69},
year = {2010}
}
@article{Costa2016,
abstract = {—In this paper, we propose a novel approach to un-supervised and online data classification. The algorithm is based on the statistical analysis of selected features and development of a self-evolving fuzzy-rule-basis. It starts learning from an empty rule basis and, instead of offline training, it learns " on-the-fly " . It is free of parameters and, thus, fuzzy rules, number, size or radius of the classes do not need to be pre-defined. It is very suitable for the classification of online data streams with real-time constraints. The past data do not need to be stored in memory, since that the algorithm is recursive, which makes it memory and computational power efficient. It is able to handle concept-drift and concept-evolution due to its evolving nature, which means that, not only rules/classes can be updated, but new classes can be created as new concepts emerge from the data. It can perform fuzzy classification/soft-labeling, which is preferred over traditional crisp classification in many areas of application. The algorithm was validated with an industrial pilot plant, where online calculated period and amplitude of control signal were used as input to a fault diagnosis application. The approach, however, is generic and can be applied to different problems and with much higher dimensional inputs. The results obtained from the real data are very significant.},
author = {Costa, Bruno Sielly Jales and Bezerra, Clauber Gomes and Guedes, Luiz Affonso and Angelov, Plamen Parvanov},
doi = {10.1109/FUZZ-IEEE.2016.7737668},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Unsupervised Classification of Data Streams based on typicality and eccentricity data analytics.pdf:pdf},
isbn = {9781509006250},
journal = {2016 IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2016},
keywords = {Autonomous learning,Classification,Clustering,Data streams,Eccentricity,Evolving systems,Fuzzy classification,Real-Time,Soft-labeling,Teda,Typicality,Unsupervised},
pages = {58--63},
title = {{Unsupervised classification of data streams based on typicality and eccentricity data analytics}},
year = {2016}
}
@article{FISHER1936,
abstract = {When two or more populations have been measured in several characters, special interest attaches to certain functions of the measurements by which the populations are discriminated. At the author's suggestion use has already been made of this fact in craniometry (a) by Mr E. S. Martin, who has applied the principle to the sex differences in measurements of the mandible, and (b) by Miss Mildred Barnard, who showed how to obtain from a series of dated series the particular compound of cranial measurements showing most distinctly a progressive or secular trend. In the present paper the application of the same principle will be illustrated on a taxonomic problem; some questions connected with the precision of the processes employed will also be discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {FISHER, R. A.},
doi = {10.1111/j.1469-1809.1936.tb02137.x},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Fisher.pdf:pdf},
isbn = {1469-1809},
issn = {20501420},
journal = {Annals of Eugenics},
keywords = {Iris data set,LDA},
number = {2},
pages = {179--188},
pmid = {334},
title = {{The Use of Multiple Measurements in Taxonomic Problems}},
url = {http://doi.wiley.com/10.1111/j.1469-1809.1936.tb02137.x},
volume = {7},
year = {1936}
}
@misc{Catlett2006b,
address = {Springer, Berlin, Heidelberg},
author = {Catlett, J},
booktitle = {Lecture Notes in Computer Science (Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {https://doi.org/10.1007/BFb0017012},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/carlet1991.pdf:pdf},
keywords = {discretisation,empirical concept learning,induction of decision trees},
pages = {164--178},
publisher = {Springer Verlag},
title = {{On changing continuous attributes into ordered discrete attributes}},
volume = {482},
year = {1991}
}
@article{Catlett2006a,
author = {Catlett, J},
file = {:home/tarcisio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Catlett - 2006 - Into Ordered Discrete Attributes(2).pdf:pdf;:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/carlet1991.pdf:pdf},
keywords = {discretisation,empirical concept learning,induction of decision trees},
number = {1989},
pages = {2006},
title = {{Into Ordered Discrete Attributes}},
volume = {3},
year = {2006}
}
@article{Bruneau2015a,
author = {Bruneau, P. and Pinheiro, P. and Broeksema, B. and Otjacques, B.},
doi = {10.1016/j.neucom.2014.09.062},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Cluster Sculptor, an interactive visual clustering system.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {feb},
pages = {627--644},
title = {{Cluster Sculptor, an interactive visual clustering system}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231214012892},
volume = {150},
year = {2015}
}
@article{Cuayahuitl2014,
abstract = {{\textcopyright} 2014 IEEE. Work on training semantic slot labellers for use in Natural Language Processing applications has typically either relied on large amounts of labelled input data, or has assumed entirely unlabelled inputs. The former technique tends to be costly to apply, while the latter is often not as accurate as its supervised counterpart. Here, we present a semi-supervised learning approach that automatically labels the semantic slots in a set of training data and aims to strike a balance between the dependence on labelled data and prediction accuracy. The essence of our algorithm is to cluster clauses based on a similarity function that combines lexical and semantic information. We present experiments that compare different similarity functions for both our semi-supervised setting and a fully unsupervised baseline. While semi-supervised learning expectedly outperforms unsupervised learning, our results show that (1) this effect can be observed based on very few training data instances and that increasing the size of the training data does not lead to better performance, and (2) that lexical and semantic information contribute differently in different domains so that clustering based on both types of information offers the best generalisation.},
author = {Cuayahuitl, Heriberto and Dethlefs, Nina and Hastie, Helen},
doi = {10.1109/ICMLA.2014.87},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/A Semi-Supervised Clustering Approach for Semantic Slot Labelling.pdf:pdf},
isbn = {9781479974153},
journal = {Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014},
keywords = {interactive systems,natural language processing,semantic slot labelling,semi-supervised learning},
pages = {500--505},
title = {{A semi-supervised clustering approach for semantic slot labelling}},
year = {2014}
}
@article{Carolina1999,
author = {Carolina, Maria and Cristiano, Ronaldo},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Aprendizado{\_}de{\_}Maquina{\_}Simbolico{\_}para{\_}Mineracao.pdf:pdf},
title = {{Aprendizado de M{\'{a}}quina Simb{\'{o}}lico de Dados para Minera{\c{c}}{\~{a}}o}},
year = {1999}
}
@article{araujo2017,
author = {Ara{\'{u}}jo, Francisco N. C.},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Rotulacao{\_}Automatica{\_}de{\_}Clusters{\_}Baseados.pdf:pdf},
isbn = {9788583202011},
journal = {X Encontro Unificado de Computa{\c{c}}{\~{a}}o - ENUCOMP 2017},
pages = {399--404},
title = {{Rotula{\c{c}}{\~{a}}o Autom{\'{a}}tica de Clusters Baseados em An{\'{a}}lise de Filogenias}},
url = {www.enucomp.com.br/2017},
volume = {91},
year = {2017}
}
@inproceedings{Yeganova2010,
abstract = {The rapid growth of biomedical literature requires accurate text analysis and text processing tools. Detecting abbreviations and identifying their definitions is an important component of such tools. In this work, we develop a machine learning algorithm for abbreviation definition identification in text. Most existing approaches for abbreviation definition identification employ rule-based methods. While achieving high precision, rule-based methods are limited to the rules defined and fail to capture many uncommon definition patterns. Supervised learning techniques, which offer more flexibility in detecting abbreviation definitions, have also been applied to the problem. However, they require manually labeled training data. In this study, we make use of what we term naturally labeled data. Positive training examples are extracted from text, which provides naturally occurring potential abbreviation-definition pairs. Negative training examples are generated randomly by mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Meds tract corpora. We achieve an F-score that is comparable to the earlier existing systems yet with a higher recall.},
author = {Yeganova, Lana and Comeau, Donald C. and Wilbur, W. John},
booktitle = {2010 Ninth International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2010.166},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/identifying Abbreviation Definitions{\_}ML with Naturally Labeled data.pdf:pdf},
isbn = {978-1-4244-9211-4},
month = {dec},
pages = {499--505},
publisher = {IEEE},
title = {{Identifying Abbreviation Definitions Machine Learning with Naturally Labeled Data}},
url = {http://ieeexplore.ieee.org/document/5708877/},
year = {2010}
}
@article{Yang2011,
abstract = {Previous semi-supervised learning (SSL) techniques usually assume unlabeled data are relevant to the target task. That is, they follow the same distribution as the targeted labeled data. In this paper, we address a different and very difficult scenario in SSL, where the unlabeled data may be a mixture of data relevant or irrelevant to the target binary classification task. In our framework, we do not require explicitly prior knowledge on the relatedness of the unla- beled data to the target data. In order to alleviate the effect of the irrelevant unlabeled data and utilize the implicit knowledge among all available data, we develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), to seek an inductive rule to separate the target binary classification task well while finding out the irrelevant data by-product. To attain this goal, we introduce a new min loss function, which can relieve the impact of the irrelevant data while relying more on the labeled data and the relevant unlabeled data. This loss function can therefore achieve the maximum entropy principle. The 3C-SVM can then generalize standard SVMs, Semi-supervised SVMs, and SVMs learned from the universum as its special cases. We further analyze the proper- ty of 3C-SVM on why the irrelevant data can help to improve the model performance. For implementation, we make relaxation and approximate the objective by the convex-concave procedure, which turns the original optimization from integral programming problem to a problem by just solving a finite number of quadratic program- ming problems. Empirical results are reported to demonstrate the advantages of our 3C-SVM model.},
author = {Yang, Haiqin and Zhu, Shenghuo and King, Irwin and Lyu, Michael R.},
doi = {10.1145/2063576.2063711},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p937-yang.pdf:pdf},
isbn = {9781450307178},
journal = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
keywords = {att,com,irwin,previous semi-supervised learning,research,ssl,techniques usually assume,that is,they follow,to the target task,unlabeled data are relevant},
pages = {937},
title = {{Can irrelevant data help semi-supervised learning, why and how?}},
url = {http://dl.acm.org/citation.cfm?doid=2063576.2063711},
year = {2011}
}
@book{runkler2012,
title={Models and Algorithms for Intelligent Data Analysis},
author={Thomas A. Runkler},
isbn={978-3-658-14075-5},
series={Data Analytics},
publisher={Springer Vieweg},
year={2016},
edition={2},
copyright={Springer Fachmedien Wiesbaden}
}
@book{breiman1984,
  title={Classification and Regression Trees},
  author={Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn={9780412048418},
  lccn={83019708},
  series={The Wadsworth and Brooks-Cole statistics-probability series},
  url={https://books.google.com.br/books?id=JwQx-WOmSyQC},
  year={1984},
  publisher={Taylor \& Francis}
}
@book{yohannes1999classification,
  title={Classification and Regression Trees, CART: A User Manual for Identifying Indicators of Vulnerability to Famine and Chronic Food Insecurity},
  author={Yohannes, Y. and Webb, P.},
  isbn={9780896293373},
  lccn={99022708},
  series={Microcomputers in policy research},
  url={https://books.google.com.br/books?id=7iuq4ikyNdoC},
  year={1999},
  publisher={International Food Policy Research Institute}
}

@incollection{Evett:1989,
 author = {Evett, I. W. and Spiehler, E. J.},
 chapter = {Rule Induction in Forensic Science},
 title = {Knowledge Based Systems},
 editor = {Duffin, P. H.},
 year = {1988},
 isbn = {0-470-21260-8},
 pages = {152--160},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=67040.67055},
 acmid = {67055},
 publisher = {Halsted Press},
 address = {New York, NY, USA},
} 

