Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{FISHER1936,
abstract = {When two or more populations have been measured in several characters, special interest attaches to certain functions of the measurements by which the populations are discriminated. At the author's suggestion use has already been made of this fact in craniometry (a) by Mr E. S. Martin, who has applied the principle to the sex differences in measurements of the mandible, and (b) by Miss Mildred Barnard, who showed how to obtain from a series of dated series the particular compound of cranial measurements showing most distinctly a progressive or secular trend. In the present paper the application of the same principle will be illustrated on a taxonomic problem; some questions connected with the precision of the processes employed will also be discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {FISHER, R. A.},
doi = {10.1111/j.1469-1809.1936.tb02137.x},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Fisher.pdf:pdf},
isbn = {1469-1809},
issn = {20501420},
journal = {Annals of Eugenics},
keywords = {Iris data set,LDA},
number = {2},
pages = {179--188},
pmid = {334},
title = {{the Use of Multiple Measurements in Taxonomic Problems}},
url = {http://doi.wiley.com/10.1111/j.1469-1809.1936.tb02137.x},
volume = {7},
year = {1936}
}
@article{Kumar2013,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities have made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval. I.},
author = {Kumar, Ashok and Andu, Thavani and Thanamani, Antony Selvdoss},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/MultidimensionalClusteringMethods.pdf:pdf},
journal = {International Journal of Engineering Science Invention},
keywords = {cluster,data mining},
mendeley-tags = {cluster,data mining},
number = {7},
pages = {1--8},
title = {{Multidimensional Clustering Methods of Data Mining for Industrial Applications}},
volume = {2},
year = {2013}
}
@misc{Metodo2015,
abstract = {Na tarefa de classifica{\c{c}}{\~{a}}o utilizando algoritmos de aprendizado de m{\'{a}}quina, considera-se a exist{\^{e}}ncia de uma base de dados chamada conjunto de treinamento. Esse conjunto possui exemplos que s{\~{a}}o rotulados(pr{\'{e}}-classificados) e utilizados no treinamento do classificador. Deve ter um total de exemplos significativo e equilibrado para que, ap{\'{o}}s o treinamento, o classificador tenha um desempenho satisfat{\'{o}}rio. Por{\'{e}}m, na maioria dos casos reais, obter esse conjunto de treinamento com a quantidade de exemplos suficientes para induzir um classificador no treinamento pode ser oneroso, pois {\'{e}} necess{\'{a}}rio que seja realizada uma rotula{\c{c}}{\~{a}}o dos dados por um especialista no problema em quest{\~{a}}o. Exemplos n{\~{a}}o-rotulados s{\~{a}}o mais f{\'{a}}ceis de serem coletados em compara{\c{c}}{\~{a}}o aos que possuem r{\'{o}}tulos. A literatura mostra o interesse da comunidade cient{\'{i}}fica em uma nova abordagem de aprendizado chamada de semissupervisionada. Este tipo de aprendizado trabalha em um cen{\'{a}}rio em que existe um conjunto de dados rotulados, insuficiente para treinar um classificador, juntamente com um outro conjunto com dados n{\~{a}}o-rotulados, tamb{\'{e}}m, dispon{\'{i}}vel no treinamento. O objetivo do trabalho {\'{e}} propor um m{\'{e}}todo que visa rotular dados a partir de um pequeno conjunto rotulado. Esse m{\'{e}}todo combina um classificador e um agrupador para realizar a tarefa de classifica{\c{c}}{\~{a}}o de forma simples em rela{\c{c}}{\~{a}}o {\`{a}} outros m{\'{e}}todos encontrados na literatura. Foram realizados experimentos utilizando 5 bases de dados e os resultados comparados com os algoritmos co-training e k-meanski, que s{\~{a}}o outros algoritmos},
address = {Teresina - PI},
author = {Lima, Bruno Vicente Alves},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Disserta{\c{c}}{\~{a}}o - Bruno Vicente - UFPI-1-1.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}} - UFPI},
keywords = {Semissupervisionado. Classifica{\c{c}}{\~{a}}o. Rotula{\c{c}}{\~{a}}o. Apr},
mendeley-tags = {Semissupervisionado. Classifica{\c{c}}{\~{a}}o. Rotula{\c{c}}{\~{a}}o. Apr},
pages = {47},
title = {{M{\'{e}}todo Semissupervisionado de Rotula{\c{c}}{\~{a}}o e Classifica{\c{c}}{\~{a}}o Utilizando Agrupamento por Sementes e Classificadores}},
type = {Disserta{\c{c}}{\~{a}}o (Programa de P{\'{o}}s-gradua{\c{c}}{\~{a}}o em Ci{\^{e}}ncia da Computa{\c{c}}{\~{a}}o)},
year = {2015}
}
@article{VonLuxburg2008,
abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
archivePrefix = {arXiv},
arxivId = {0810.4752},
author = {von Luxburg, Ulrike and Schoelkopf, Bernhard},
doi = {10.1016/B978-0-444-52936-7.50016-1},
eprint = {0810.4752},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/statisticalLearningTheory.pdf:pdf},
issn = {18745857},
journal = {Learning},
pages = {1--40},
title = {{Statistical Learning Theory: Models, Concepts, and Results}},
url = {http://arxiv.org/abs/0810.4752},
year = {2008}
}
@article{ENS2001,
abstract = {O presente artigo relata a proposta na PUCPR de unir pesquisa e doc{\^{e}}ncia na forma{\c{c}}{\~{a}}o continuada de professores, mais especificamente dos estudos sobre a abordagem qualitativa e a metodologia da pesquisa que est{\'{a}} sendo utilizada no projeto Gest{\~{a}}o Estrat{\'{e}}gica de Compet{\^{e}}ncias e a Forma{\c{c}}{\~{a}}o do Professor. A realiza{\c{c}}{\~{a}}o da pesquisa vem propiciando ao grupo de professores da {\'{a}}rea de educa{\c{c}}{\~{a}}o um trabalho integrado, e ao mesmo tempo, um caminhar pela pes- quisa-a{\c{c}}{\~{a}}o, integrando os seguintes procedimentos e t{\'{e}}cnicas de pesquisa: an{\'{a}}- lise documental, an{\'{a}}lise iconogr{\'{a}}fica, aplica{\c{c}}{\~{a}}o de question{\'{a}}rios, entrevista semi-estruturada, observa{\c{c}}{\~{a}}o participante e semin{\'{a}}rios},
author = {ENS, Romilda Teodora and Ploharski, Nara Regina and SALLES, Suely Therezinha Costa},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dialogo-740.pdf:pdf},
journal = {Revista Di{\'{a}}logo Educacional},
keywords = {metodologia da pesquisa,pesquisa,pesquisa-a{\c{c}}{\~{a}}o,procedi- mentos,t{\'{e}}cnicas de pesquisa},
number = {4},
pages = {67--84},
title = {{a Pesquisa E O Fazer Pedag{\'{o}}gico : Gerar E DIFUNDIR CONHECIMENTOS}},
volume = {2},
year = {2001}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Algumas{\_}informacoes{\_}uteis{\_}sobre{\_}MachineLearning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@book{Wu2008,
abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, k NN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development. {\textcopyright} Springer-Verlag London Limited 2007. },
author = {Wu, Xindong and Kumar, Vipin and Ross, Quinlan J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
booktitle = {Knowledge and Information Systems},
doi = {10.1007/s10115-007-0114-2},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/10Algorithms-08.pdf:pdf},
isbn = {1011500701},
issn = {02191377},
number = {1},
pages = {1--37},
pmid = {25720841},
title = {{Top 10 algorithms in data mining}},
volume = {14},
year = {2008}
}
@article{Yang2011,
abstract = {Previous semi-supervised learning (SSL) techniques usually assume unlabeled data are relevant to the target task. That is, they follow the same distribution as the targeted labeled data. In this paper, we address a different and very difficult scenario in SSL, where the unlabeled data may be a mixture of data relevant or irrelevant to the target binary classification task. In our framework, we do not require explicitly prior knowledge on the relatedness of the unla- beled data to the target data. In order to alleviate the effect of the irrelevant unlabeled data and utilize the implicit knowledge among all available data, we develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), to seek an inductive rule to separate the target binary classification task well while finding out the irrelevant data by-product. To attain this goal, we introduce a new min loss function, which can relieve the impact of the irrelevant data while relying more on the labeled data and the relevant unlabeled data. This loss function can therefore achieve the maximum entropy principle. The 3C-SVM can then generalize standard SVMs, Semi-supervised SVMs, and SVMs learned from the universum as its special cases. We further analyze the proper- ty of 3C-SVM on why the irrelevant data can help to improve the model performance. For implementation, we make relaxation and approximate the objective by the convex-concave procedure, which turns the original optimization from integral programming problem to a problem by just solving a finite number of quadratic program- ming problems. Empirical results are reported to demonstrate the advantages of our 3C-SVM model.},
author = {Yang, Haiqin and Zhu, Shenghuo and King, Irwin and Lyu, Michael R.},
doi = {10.1145/2063576.2063711},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p937-yang.pdf:pdf},
isbn = {9781450307178},
journal = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
keywords = {att,com,irwin,previous semi-supervised learning,research,ssl,techniques usually assume,that is,they follow,to the target task,unlabeled data are relevant},
pages = {937},
title = {{Can irrelevant data help semi-supervised learning, why and how?}},
url = {http://dl.acm.org/citation.cfm?doid=2063576.2063711},
year = {2011}
}
@misc{Raimundo2008,
abstract = {O avan{\c{c}}o computacional no que se refere ao processamento earmazenamento contribuiu para a forma{\c{c}}{\~{a}}o de grandes reposit{\'{o}}rios de dados,tornando-se necess{\'{a}}rio o desenvolvimento de tecnologias destinadas {\`{a}} an{\'{a}}lisede informa{\c{c}}{\~{o}}es e obten{\c{c}}{\~{a}}o de novos conhecimentos. Dentre essas tecnologiaso data mining constitui-se como uma das alternativas, utilizando para issoalgoritmos. Este artigo apresenta a modelagem matem{\'{a}}tica e implementa{\c{c}}{\~{a}}odo algoritmo CART para indu{\c{c}}{\~{a}}o de {\'{a}}rvores de decis{\~{a}}o na tarefa declassifica{\c{c}}{\~{a}}o do processo de data mining em uma Shell denominada de Orion.},
author = {Raimundo, Lidiane Rosso and Mattos, Merisandra C{\^{o}}rtes De and Waleska, Priscyla},
file = {:home/tarcisio/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/1994-6024-1-PB.pdf:pdf},
keywords = {an{\'{a}}lise,armazenamento contribuiu para a,cart algorithm,classification,data mining,de dados,de tecnologias destinadas {\`{a}},decision trees,forma{\c{c}}{\~{a}}o de grandes reposit{\'{o}}rios,o avan{\c{c}}o computacional no,processamento e,que se refere ao,resumo,tornando-se necess{\'{a}}rio o desenvolvimento},
title = {{O Algoritmo de Classifica{\c{c}}{\~{a}}o CART em uma Ferramenta de Data Mining}},
year = {2008}
}
@misc{LOPES2014,
abstract = {O problema de agrupamento (clustering) tem sido considerado como um dos problemas mais relevantes dentre aqueles existentes na {\'{a}}rea de pesquisa de aprendizagem n{\~{a}}o- supervisionada (sub{\'{a}}rea de Aprendizagem de M{\'{a}}quina). Embora o desenvolvimento e aprimoramento de algoritmos que solucionam esse problema tenha sido o principal foco de muitos pesquisadores o objetivo inicial se manteve obscuro: a compreens{\~{a}}o dos grupos formados. T{\~{a}}o importante quanto a identifica{\c{c}}{\~{a}}o dos grupos (clusters) {\'{e}} sua compreens{\~{a}}o e defini{\c{c}}{\~{a}}o. Uma boa defini{\c{c}}{\~{a}}o de um cluster representa um entendimento significativo e pode ajudar o especialista ao estudar ou interpretar dados. Frente ao problema de compreender clusters – isto {\'{e}}, de encontrar uma defini{\c{c}}{\~{a}}o ou em outras palavras, um r{\'{o}}tulo – este trabalho apresenta uma defini{\c{c}}{\~{a}}o para esse problema, deno- minado problema de rotula{\c{c}}{\~{a}}o, al{\'{e}}m de uma solu{\c{c}}{\~{a}}o baseada em t{\'{e}}cnicas com aprendi- zagem supervisionada, n{\~{a}}o-supervisionada e um modelo de discretiza{\c{c}}{\~{a}}o. Dessa forma, o problema {\'{e}} tratado desde sua concep{\c{c}}{\~{a}}o: o agrupamento de dados. Para isso, um m{\'{e}}todo com aprendizagem n{\~{a}}o-supervisionada {\'{e}} aplicado ao problema de clustering e ent{\~{a}}o um algoritmo com aprendizagem supervisionada ir{\'{a}} detectar quais atributos s{\~{a}}o relevantes para definir um dado cluster. Adicionalmente, algumas estrat{\'{e}}gias s{\~{a}}o utilizadas para formar uma metodologia que apresenta em sua totalidade um r{\'{o}}tulo (baseado em atributos e valores) para cada grupo fornecido. Finalmente, essa metodo- logia {\'{e}} aplicada em quatro bases de dados distintas apresentando bons resultados com uma m{\'{e}}dia acima de 93.5{\%} dos elementos rotulados corretamente..},
address = {Teresina},
author = {Lopes, Lucas A.},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dissertacao{\_}lopes.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}} - UFPI},
keywords = {agrupamento,aprendizado de m{\'{a}}quina,rotula{\c{c}}{\~{a}}o},
mendeley-tags = {agrupamento,aprendizado de m{\'{a}}quina,rotula{\c{c}}{\~{a}}o},
pages = {73},
title = {{Rotula{\c{c}}{\~{a}}o Autom{\'{a}}tica de Grupos com Aprendizagem de M{\'{a}}quina Supervisionada}},
type = {Disserta{\c{c}}{\~{a}}o (Programa de P{\'{o}}s-gradua{\c{c}}{\~{a}}o em Ci{\^{e}}ncia da Computa{\c{c}}{\~{a}}o)},
year = {2014}
}
@article{Dougherty1995,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
address = {Stanford},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
doi = {10.1016/B978-1-55860-377-6.50032-3},
eprint = {9809069v1},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/supervisedUnsupervisedDiscretization.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Machine Learning Proceedings 1995},
pages = {194--202},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Supervised and Unsupervised Discretization of Continuous Features}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603776500323},
volume = {0},
year = {1995}
}
@article{Hwang2002,
abstract = {A discretization technique converts continuous attribute values into discrete ones. Discretization is needed when classification algorithms require only discrete attributes. It is also useful to increase the speed and the accuracy of classification algorithms. This paper presents a dynamic discretization method, whose main characteristic is to detect interdependencies between all continuous attributes. Empirical evaluation on 12 datasets from the UCI repository shows that the proposed algorithm is a relatively effective method for discretization.},
author = {Hwang, Grace J and Li, Fumin},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/dynamicMethodforDiscretizationofContinuousAttributes.pdf:pdf},
isbn = {9783540440253},
issn = {16113349},
journal = {Lecture Notes in Computer Science - Intelligent Data Engineering and Automated Learning - IDEAL 2002: Third International Conference},
pages = {506},
title = {{A Dynamic Method for Discretization of Continuous Attributes}},
url = {http://www.springerlink.com/content/4n05b2n6x0cx4tlk},
volume = {2412/2002},
year = {2002}
}
@inproceedings{Lucca2013,
abstract = {Este artigo apresenta uma ferramenta para classifica{\c{c}}{\~{a}}o de texto baseada no algoritmo Na{\"{i}}ve Bayes. S{\~{a}}o descritos alguns conceitos b{\'{a}}sicos sobre classifica{\c{c}}{\~{a}}o textual na {\'{a}}rea Recupera{\c{c}}{\~{a}}o de Informa{\c{c}}{\~{o}}es, o algoritmo escolhido, um exemplo de utiliza{\c{c}}{\~{a}}o e a arquitetura da ferramenta.},
address = {Rio Grande - RS},
author = {Lucca, G and Pereira, I A and Prisco, A and Borges, E N},
booktitle = {IX Escola Regional de Banco de Dados – ERBD 2013},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/0019.pdf:pdf},
keywords = {naive bayes},
mendeley-tags = {naive bayes},
organization = {Centro de Ci{\^{e}}ncias Computacionais da Universidade Federal do Rio Grande},
pages = {1--4},
title = {{Uma implementa{\c{c}}{\~{a}}o do algoritmo Na{\"{i}}ve Bayes para classifica{\c{c}}{\~{a}}o de texto}},
url = {http://ifc-camboriu.edu.br/erbd2013},
year = {2013}
}
@article{Loh2010,
author = {Loh, Wei-Yin},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/loh{\_}slides{\_}IRIS{\_}CARS.pdf:pdf},
journal = {Technicalities},
pages = {18},
title = {{A Brief History of Classification}},
url = {http://web.ebscohost.com.pitt.idm.oclc.org/ehost/pdfviewer/pdfviewer?vid=111{\&}sid=3ba6f675-4abc-4b03-bf5b-badf431d95cc@sessionmgr14{\&}hid=26},
year = {2010}
}
@article{Mccallum1997,
author = {Mccallum, Andrew and Nigam, Kamal},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/multinomial-aaaiws98.pdf:pdf},
title = {{A Comparison of Event Models for Naive Bayes Text Classification}},
year = {1997}
}
@book{Mohri2012,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Mehryar{\_}Mohri{\_}Afshin{\_}Rostamizadeh{\_}Ameet{\_}Talwalkar(BookFi.org).pdf:pdf},
isbn = {9780262018258},
title = {{Foundations Machine Learning}},
year = {2012}
}
@phdthesis{Edmar1999,
author = {Edmar, Martineli},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Tese{\_}Edmar.pdf:pdf},
pages = {1--104},
title = {{Extra{\c{c}}{\~{a}}o de conhecimento de redes neurais artificiais}},
year = {1999}
}
@article{kotsiantis2005logitboost,
abstract = {The ensembles of simple Bayesian classifiers have traditionally not been a focus of research. The reason is that simple Bayes is an extremely stable learning algorithm and most ensemb le techniques such as bagging is mainly variance reduction techniques, thus not being able to benefit from its integration. However, simple Bayes can be effectively used in ensemble techniques, which perform also bias reduction, such as Logitboost. Ho wever, Logitboost requires a regre ssion algorithm for base learner. For this reason, we slightly modify simple Bayesian cl assifier in order to be able to run as a regression method. Finally, we performed a large-scale comparison on 27 standard benchmark datasets with other state-of-the-art algorithms and ensembles using the simple Bayesian algorithm as base learner and the proposed technique was more accurate in most cases.},
address = {Patras},
author = {Kotsiantis, S B and Pintelas, P E},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/logitboostOfSimpleBayesianClassifier.pdf:pdf},
keywords = {2004,bayesian classifiers have traditionally,ensemble techniques such as,is an extremely stable,is that simple bayes,learning algorithm and most,not been a focus,november 30,of research,predictive data mining,received,supervised machine learning,the ensembles of simple,the reason},
pages = {53--59},
title = {{Logitboost of Simple Bayesian Classifier}},
volume = {29},
year = {2005}
}
@article{Salazar2003,
author = {Salazar, Luiz Filipe},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/DISCRETIZACAO{\_}APRENDIZAGEM{\_}BAYESINA.pdf:pdf},
journal = {Ele.Ita.Br},
title = {{Discretiza{\c{c}}{\~{a}}o para Aprendizagem Bayesiana: Aplica{\c{c}}{\~{a}}o no Aux{\'{i}}lio {\`{a}} Valida{\c{c}}{\~{a}}o de Dados em Prote{\c{c}}{\~{a}}o ao V{\^{o}}o}},
url = {http://www.ele.ita.br/{~}jackson/files/msc.pdf},
year = {2003}
}
@book{Barber2011,
abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Barber, David},
doi = {10.1017/CBO9780511804779},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Bayesian{\_}reasoning{\_}and{\_}Machine{\_}learning{\_}DAVID{\_}BARBER.pdf:pdf},
isbn = {9780511804779},
issn = {9780521518147},
pmid = {16931139},
title = {{Bayesian Reasoning and Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779},
year = {2011}
}
@misc{Filho2015,
abstract = {O agrupamento (clustering) de dados tem sido considerado como um dos tópicos mais rele- vantes dentre aqueles existentes na área de aprendizagem de máquina não-supervisionada. Embora o desenvolvimento e aprimoramento de algoritmos que tratam esse problema tenham sido o principal foco de muitos pesquisadores, a compreensão da definição dos grupos (clusters) é tão importante quanto sua formação. Uma boa definição de um grupo pode ajudar na interpretação dos dados. Frente ao problema de compreender a definição dos grupos este trabalho descreve uma solução que utiliza a teoria de conjuntos fuzzy para identificar os elementos mais relevantes do agrupamento e modelar faixas de valores que sejam capazes de identificar cada um dos grupos, baseando-se em caracterı́sticas únicas. Os experimentos realizados demostram que o modelo proposto é bastante factı́vel e capaz de construir faixas de valores para a identificação dos grupos, assim como classificar novos elementos utilizando as definições fornecidas.},
address = {Teresina},
author = {Filho, Vilmar P. R.},
file = {:home/tarcisio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Filho - 2015 - Rotulação de grupos utilizando conjuntos fuzzy.pdf:pdf},
institution = {Universidade Federal do Piau{\'{i}}},
keywords = {Cluster,Fuzzy,Labeling,Learning (artificial intelligence),fuzzy,rotula{\c{c}}{\~{a}}o},
mendeley-tags = {fuzzy,rotula{\c{c}}{\~{a}}o},
pages = {25},
title = {{Rotulação de grupos utilizando conjuntos fuzzy}},
type = {Disserta{\c{c}}{\~{a}}o (Programa de P{\'{o}}s-gradua{\c{c}}{\~{a}}o em Ci{\^{e}}ncia da Computa{\c{c}}{\~{a}}o)},
year = {2015}
}
@article{Iria2009,
abstract = {We present an approach to automating knowledge extraction in the aerospace engineering domain which has had a fundamental impact on the way engineers manage their collective knowledge built with years of experience. Even though obtaining labelled data in this domain is hard due to the high cost of domain experts' time, the application of the machine learning-based technology was successful, yielding results comparable to the state-of-the-art. Moreover, we present a comparison between several machine learning approaches in extracting knowledge from reports about jet engines. We show that the application of a semi-supervised approach does not provide a significant increase in accuracy so as to justify its adoption due to its much higher computational cost, but that the application of a large-scale approach considerably reduces both training and testing time while keeping accuracy comparable to the standard supervised approach, making it a good choice for this class of application scenarios.},
author = {Iria, Jos{\'{e}}},
doi = {10.1145/1597735.1597753},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p97-iria.pdf:pdf},
isbn = {9781605586588},
journal = {Proceedings of the fifth international conference on Knowledge capture - K-CAP '09},
keywords = {aerospace,information extraction,knowledge capture,machine learning},
pages = {97--104},
title = {{Automating knowledge capture in the aerospace domain}},
url = {http://dl.acm.org/citation.cfm?id=1597735.1597753},
year = {2009}
}
@article{Sanches2003,
abstract = {In order to apply machine learning classification algorithms, it is assumed that there exists a set of labeled data, called the training set, which is used to train the classifier. However, in real life, this training set may not contain enough labeled data to induce a good classifier. Recently, there has been much interest in a variant of this approach. This new approach, known as semi-supervised learning, assumes that, in addition to the labeled training data, there exists a second set of unlabeled data which is also available during training. One of the goals of semi-supervised learning is training classifiers when large amounts of unlabeled data are available together with a small amount of labeled data. The appeal of semi-supervised learning is due to the fact that in many real-world applications, such sets of unlabeled data are either readily available or inexpensive to collect compared to labeled data. Unlabeled data can often be collected by auto- mated means while labeled data requires human experts or other limited or expensive classification resources that may even be unfeasible in some cases. There are several ways in which unlabeled data can be used. In this work we explore one mechanism by which unlabeled data can be used to improve classification problems and propose a semi-supervised algorithm, called k-meanski, for using unlabeled data for supervised learning. There are two premises underlying the technique used by the proposed algorithm. The first is that input data falls naturally into clusters rather than being uniformly distributed across the entire input space. Furthermore, the initial input labeled data should fall near the center of the existing clusters in the input space. The second premise is that many of the points in some of these clusters belong to specific output categories. Obviously, the validity of these premises is dependent on the dataset used. The k-meanski algorithm works well when the data conform to both premises. If these assumptions are violated poor performance can result. Experiments using real world datasets where we randomly select a subset of the available data to act as labeled exemplars are shown.},
author = {Sanches, Marcelo Kaminski},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Dissertacao{\_}MKS.pdf:pdf},
title = {{Aprendizado de m{\'{a}}quina semi-supervisionado: proposta de um algoritmo para rotular exemplos a partir de poucos exemplos rotulados}},
year = {2003}
}
@article{Steiner2006,
abstract = {A “Descoberta de Conhecimento em Bases de Dados” (Knowledge Discovery in Databases, KDD) {\'{e}} um processo composto de v{\'{a}}rias etapas, iniciando com a coleta de dados para o problema em pauta e finalizando com a interpreta- {\c{c}}{\~{a}}o e avalia{\c{c}}{\~{a}}o dos resultados obtidos. O presente trabalho objetiva mostrar a influ{\^{e}}ncia da an{\'{a}}lise explorat{\'{o}}ria dos dados no desempenho das t{\'{e}}cnicas de Minera{\c{c}}{\~{a}}o de Dados (Data Mining) quanto {\`{a}} classifica{\c{c}}{\~{a}}o de novos padr{\~{o}}es por meio da sua aplica{\c{c}}{\~{a}}o a um problema m{\'{e}}dico, al{\'{e}}m de comparar o desempenho delas entre si, visando obter a t{\'{e}}cnica com o maior percentual de acertos. Pelos resultados obtidos, pode-se concluir que a referida an{\'{a}}lise, se conduzida de forma adequada, pode trazer importantes melhorias nos desempenhos de quase todas as t{\'{e}}cnicas abor- dadas, tornando-se, assim, uma importante ferramenta para a otimiza{\c{c}}{\~{a}}o dos resultados finais. Para o problema em estudo, a t{\'{e}}cnica que envolve um modelo de Programa{\c{c}}{\~{a}}o Linear e uma outra que envolve Redes Neurais foram as t{\'{e}}cnicas que apresentaram os menores percentuais de erros para os conjuntos de testes, apresentando capacidades de generaliza{\c{c}}{\~{a}}o satisfat{\'{o}}rias.},
author = {Steiner, Maria Teresinha Arns MTA and Soma, Nei Yoshihiro NY and Shimizu, Tamio and Nievola, J{\'{u}}lio Cesar and steiner Neto, Pedro Jos{\'{e}}},
doi = {10.1590/S0104-530X2006000200013},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/31177{\_}problema{\_}medico{\_}processo{\_}kdd{\_}analise{\_}exploratoria{\_}dados.pdf:pdf},
issn = {0104-530X},
journal = {Gest{\~{a}}o {\&} Produ{\c{c}}{\~{a}}o},
keywords = {an{\'{a}}lise explorat{\'{o}}ria dos dados.,minera{\c{c}}{\~{a}}o de dados,processo KDD},
pages = {325--337},
title = {{Abordagem de um problema m{\'{e}}dico por meio do processo de KDD com {\^{e}}nfase {\`{a}} an{\'{a}}lise explorat{\'{o}}ria dos dados.}},
url = {http://www.scielo.br/scielo.php?script=sci{\_}arttext{\&}pid=S0104-530X2006000200013{\&}nrm=iso{\%}5Cnhttp://www.scielo.br/pdf/gp/v13n2/31177.pdf},
volume = {13},
year = {2006}
}
@misc{Federal2016,
author = {Federal, Universidade and Piau{\'{i}}, D O and Propesq, Pr{\'{o}}-reitoria D E Pesquisa},
title = {{Processo de Descoberta de Conhecimento em Base de Dados para previs{\~{a}}o de ocorr{\^{e}}ncias de Esp{\'{e}}cimes de Peixe Boi Marinho}},
year = {2016}
}
@article{Tatibana,
author = {Tatibana, Cassia Yuri and Kaetsu, Deisi Yuki},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/classificacao{\_}automatica{\_}artigos{\_}utilizando{\_}redes{\_}neurais.pdf:pdf},
title = {{Redes Neurais}},
url = {http://www.din.uem.br/ia/neurais/{\#}bibliografia}
}
@article{Method1992,
author = {Method, Distance-based Discretization and Cerquides, Jesus and Mantaras, Ramon Lopez De},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/1997-Cerquides-kdd.pdf:pdf},
journal = {Kdd},
pages = {139--142},
title = {{Proposal and Empirical Comparison of a Parallelizable}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.7428},
year = {1992}
}
@article{Frank1999,
abstract = {Before applying learning algorithms to datasets, practitioners often globally discretize any numeric attributes. If the algorithm cannot handle numeric attributes directly, prior discretization is essential. Even if it can, prior discretization often accelerates induction, and may produce simpler and more accurate classifiers. As it is generally done, global discretization denies the learning algorithm any chance of taking advantage of the ordering information implicit in numeric attributes. However, a simple transformation of discretized data preserves this information in a form that learners can use. We show that, compared to using the discretized data directly, this transformation significantly increases the accuracy of decision trees built by C4.5, decision lists built by PART, and decision tables built using the wrapper method, on several bench-mark datasets. Moreover, it can significantly reduce the size of the resulting classifiers. This simple technique makes global discretization an even more useful tool for data preprocessing},
author = {Frank, Eibe and Witten, Ian H.},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/99EF-IHW-Globe-Discre.pdf:pdf},
keywords = {computer science,discretization},
pages = {1--12},
title = {{Making better use of global discretization}},
url = {http://researchcommons.waikato.ac.nz/handle/10289/1507},
year = {1999}
}
@article{Bruce2001,
author = {Bruce, RF},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/10.1.1.58.986.pdf:pdf},
journal = {Nlprs},
keywords = {Bayesian,semi-class},
title = {{A Bayesian Approach to Semi-Supervised Learning}},
url = {http://www.researchgate.net/publication/2930623{\_}A{\_}Bayesian{\_}Approach{\_}to{\_}Semi-Supervised{\_}Learning/file/9c96051b000243458e.pdf},
year = {2001}
}
@book{Bishop2013,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2013}
}
@article{Charytanowicz2010,
abstract = {Methods based on kernel density estimation have been successfully applied for various data mining techniques. Their natural interpretation together with consistency properties make them an attractive tool in clustering problems. In this paper, the complete gradient clustering algorithm, based on the density of the data, is presented. The proposed method has been applied to a real data set of grains and compared with K-means clustering algorithm. The wheat varieties, Kama, Rosa and Canadian, characterized by measurements of main grain geometric features obtained by X-ray technique, have been analyzed. Results indicate that the proposed method is expected to be an effective method for recognizing wheat varieties. Moreover, it outperforms the K-means analysis if the nature of the grouping structures among the data is unknown before processing.},
author = {Charytanowicz, Ma{\l}gorzata and Niewczas, Jerzy and Kulczycki, Piotr and Kowalski, Piotr A. and {\L}ukasik, Szymon and Zak, S{\l}awomir},
doi = {10.1007/978-3-642-13105-9_2},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Charytanowicz{\_}et{\_}al A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images.pdf:pdf},
isbn = {9783642131042},
issn = {18675662},
journal = {Advances in Intelligent and Soft Computing},
pages = {15--24},
title = {{Complete gradient clustering algorithm for features analysis of X-ray images}},
volume = {69},
year = {2010}
}
@article{Lorenzett2016,
author = {Lorenzett, Cassio dal Castel and Teloken, Alex},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/4023-10934-1-PB.pdf:pdf},
journal = {Simp{\'{o}}sio de Pesquisa e Desenvolvimento em Computa{\c{c}}{\~{a}}o},
number = {1},
title = {{Estudo Comparativo entre os algoritmos de Minera{\c{c}}{\~{a}}o de Dados Random Forest e J48 na tomada de Decis{\~{a}}o}},
volume = {2},
year = {2016}
}
@article{Kotsiantis2006,
abstract = {A discretization algorithm is needed in order to handle problems with real-valued attributes with Decision Trees (DTs), Bayesian Networks (BNs) and Rule-Learners (RLs), treating the resulting intervals as nominal val- ues. The performance of these systems is tied to the right election of these in- tervals. A good discretization algorithm has to balance the loss of information intrinsic to this kind of process and generating a reasonable number of cut points, that is, a reasonable search space. This paper presents the well known discretization techniques. Of course, a single article cannot be a complete re- view of all discretization algorithms. Despite this, we hope that the references cited cover the major theoretical issues and guide the researcher to interesting research directions and suggest possible combinations that have to be explored.},
author = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris},
doi = {10.1016/B978-044452781-3/50006-2},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/discretizationTechniques.pdf:pdf},
isbn = {9780444527813},
journal = {GESTS International Transactions on Computer Science and Engineering},
number = {1},
pages = {47--58},
title = {{Discretization Techniques : A recent survey}},
volume = {32},
year = {2006}
}
@article{Quinlan1986,
abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
author = {Quinlan, J. R.},
doi = {10.1023/A:1022643204877},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/quinlan.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {classification,decision trees,expert systems,induction,information theory,knowledge acquisition},
number = {1},
pages = {81--106},
pmid = {17050186},
title = {{Induction of Decision Trees}},
volume = {1},
year = {1986}
}
@book{Montgomery2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Montgomery, Karen},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
edition = {1},
editor = {{O'Reilly Media}, Inc.},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/big-data-now-2012.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
publisher = {O'Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472},
title = {{Big Data Now}},
volume = {53},
year = {2013}
}
@misc{Catlett2006b,
address = {Springer, Berlin, Heidelberg},
author = {Catlett, J},
booktitle = {Lecture Notes in Computer Science (Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {https://doi.org/10.1007/BFb0017012},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/carlet1991.pdf:pdf},
keywords = {discretisation,empirical concept learning,induction of decision trees},
pages = {164--178},
publisher = {Springer Verlag},
title = {{On changing continuous attributes into ordered discrete attributes}},
volume = {482},
year = {1991}
}
@book{Pyle1999,
abstract = {Data preparation is a fundamental stage of data analysis. While a lot of low-quality information is available in various data sources and on the Web, many organizations or companies are interested in how to transform the data into cleaned forms which can be used for high-profit purposes. This goal generates an urgent need for data analysis aimed at cleaning the raw data. In this paper, we first show the importance of data preparation in data analysis, then introduce some research achievements in the area of data preparation. Finally, we suggest some future directions of research and development. Data preparation is a fundamental stage of data analysis. While a lot of low-quality information is available in various data sources and on the Web, many organizations or companies are interested in how to transform the data into cleaned forms which can be used for high-profit purposes. This goal generates an urgent need for data analysis aimed at cleaning the raw data. In this paper, we first show the importance of data preparation in data analysis, then introduce some research achievements in the area of data preparation. Finally, we suggest some future directions of research and development.},
author = {Pyle, Dorian and Editor, Senior and Cerra, Diane D},
booktitle = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
doi = {10.1080/713827180},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/DataPreparationForDataMining-DorianPyle.pdf:pdf},
isbn = {4159822665},
issn = {08839514},
keywords = {counter-terrorism,data mining,privacy},
pages = {375--381},
pmid = {4047876},
title = {{Data Preparation for Data Mining}},
url = {http://www.tandfonline.com/doi/abs/10.1080/713827180},
volume = {17},
year = {1999}
}
@inproceedings{Pq,
abstract = {When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.},
address = {Montr{\'{e}}al, Qu{\'{e}}, Canada},
author = {George, H. John and Langley, Pat},
booktitle = {UAI'95 Proceedings of the Eleventh conference on Uncertainty in artificial intelligence},
editor = {Besnard, Philippe (IRISA Rennes / France) and Hanks, Steve (University of Washington / Seattle)},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/artigo{\_}bayes-continuous.pdf:pdf},
keywords = {bayes},
mendeley-tags = {bayes},
pages = {338--345},
publisher = {Morgan Kalfmann Publishers},
title = {{Estimating Continuous Distributions in Bayesian Classifiers}},
url = {http://dl.acm.org/citation.cfm?id=2074158.2074196},
year = {1995}
}
@book{RusselStuart.Norvig2013,
address = {Rio de Janeiro},
author = {Russel, Stuart and Norvig, Peter},
edition = {3{\textordfeminine}},
editor = {Ltda, Elsevier Editora},
isbn = {9780136042594},
title = {{Intelig{\^{e}}ncia Artificial}},
year = {2013}
}
@article{Carolina1999,
author = {Carolina, Maria and Cristiano, Ronaldo},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Aprendizado{\_}de{\_}Maquina{\_}Simbolico{\_}para{\_}Mineracao.pdf:pdf},
title = {{Aprendizado de M{\'{a}}quina Simb{\'{o}}lico de Dados para Minera{\c{c}}{\~{a}}o}},
year = {1999}
}
@article{Trandafili2012,
abstract = {Higher education institutions are overwhelmed with huge amounts of information regarding student's enrollment, number of courses completed, achievement in each course, performance indicators and other data. This has led to an increasingly complex analysis process of the growing volume of data and to the incapability to take decisions regarding curricula reform and restructuring. On the other side, educational data mining is a growing field aiming at discovering knowledge from student's data in order to thoroughly understand the learning process and take appropriate actions to improve the student's performance and the quality of the courses delivery. This paper presents a thorough analysis process performed on student's data through machine learning techniques. Experiments performed on a very large real-world dataset of students performance on all courses of a university, reveal interesting and important students profiles with clustering and surprising relationships among the courses performance with association},
author = {Trandafili, Evis and Allko{\c{c}}i, Alban and Kajo, Elinda and Xhuvani, Aleksand{\"{e}}r},
doi = {10.1145/2371316.2371350},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p174-trandafili.pdf:pdf},
isbn = {9781450312400},
journal = {Proceedings of the Fifth Balkan Conference in Informatics on - BCI '12},
pages = {174},
title = {{Discovery and evaluation of student's profiles with machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2371316.2371350},
year = {2012}
}
@article{Yohannes1999,
author = {Yohannes, Yisehac and Hoddinott, John},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Pnach725{\_}CART.pdf:pdf},
pages = {1--29},
title = {{Classification and regression trees: an introduction}},
year = {1999}
}
@phdthesis{Madureira2017,
abstract = {Um grande n´ umero de mensagens curtas informais s˜ ao postadas dia- riamente em redes sociais, f´ orums de discuss˜ ao e pesquisas de satisfa¸ao. c˜ Emo¸oes parecem ser importantes de forma frequente nesses textos. O de- c˜ safio de identificar e entender a emo¸ao presente nesse tipo de comunica¸aoc˜ c˜ ´ e importante para distinguir o sentimento presente no texto e tamb´ em para identificar comportamentos anˆ omalos e inapropriados, eventualmente ofere- cendo algum tipo de risco. Este trabalho prop˜ oe a implementa¸ao de uma solu¸ao para a an´ c˜ c˜ alise de sentimento de textos curtos baseada em aprendizado por m´ aquina. Utili- zando t´ ecnicas de aprendizado supervisionado, ´ e desejado discernir se uma mensagem possui sentimento positivo, neutro ou negativo. As mensagens a serem analisadas ser˜ ao pesquisas de satisfa¸ao de servi¸ c˜ cos de TI. Foram uti- lizados nas an´ alises dois modelos, o primeiro modelo onde apenas o campo de texto livre ”Coment´ ario”foi considerado e o segundo modelo, onde al´ em do campo de texto livre ”Coment´ ario”, foram consideradas, adicionalmente, duas perguntas objetivas da pesquisa de satisfa¸ao. c˜ Os resultados obtidos indicam que as t´ ecnicas utilizadas de aprendizado por m´ aquina, n˜ ao ficam atr´ as dos resultados produzidos por aprendizado humano. A acur´ acia obtida foi de at´ e 86,8{\%}de acerto para ummodelo de trˆ es classes: ”elogio”, ”neutro”e ”reclama¸ao”. A acur´ c˜ acia foi significativamente superior, alcan¸ cando at´ e 94,5{\%} em um modelo alternativo, de apenas duas classes: ”elogio”e ”n˜ ao-elogio”},
address = {Rio de Janeiro},
author = {Madureira, Daniel Fialho},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/FGV EMAp - Gustavo Avila - An{\'{a}}lise de Sentimento para Textos Curtos.pdf:pdf},
school = {Fundacao Getulio Vargas},
title = {{Analise de sentimento para textos curtos}},
year = {2017}
}
@article{Chang2016,
author = {Chang, Shuo and Dai, Peng and Hong, Lichan and Sheng, Cheng and Zhang, Tianjiao and Chi, Ed H.},
doi = {10.1145/2856767.2856783},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p348-chang.pdf:pdf},
isbn = {9781450341370},
journal = {Proceedings of the 21st International Conference on Intelligent User Interfaces - IUI '16},
keywords = {clustering,interactive machine learning,search result clustering},
pages = {348--358},
title = {{AppGrouper: Knowledge-graph-based Interactive Clustering Tool for Mobile App Search Results}},
url = {http://dl.acm.org/citation.cfm?doid=2856767.2856783},
year = {2016}
}
@article{Hwang2002a,
abstract = {A discretization technique converts continuous attribute values into discrete ones. Discretization is needed when classification algorithms require only discrete attributes. It is also useful to increase the speed and the accuracy of classification algorithms. This paper presents a dynamic discretization method, whose main characteristic is to detect interdependencies between all continuous attributes. Empirical evaluation on 12 datasets from the UCI repository shows that the proposed algorithm is a relatively effective method for discretization.},
author = {Hwang, Grace J and Li, Fumin},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Hwang2002.pdf:pdf},
isbn = {9783540440253},
issn = {16113349},
journal = {Lecture Notes in Computer Science - Intelligent Data Engineering and Automated Learning - IDEAL 2002: Third International Conference},
pages = {506},
title = {{A Dynamic Method for Discretization of Continuous Attributes}},
url = {http://www.springerlink.com/content/4n05b2n6x0cx4tlk},
volume = {2412/2002},
year = {2002}
}
@article{Steiner2007,
abstract = {A avalia{\c{c}}{\~{a}}o de risco de cr{\'{e}}dito {\'{e}} um importante problema administrativo da {\'{a}}rea de an{\'{a}}lise financeira. As Redes Neurais t{\^{e}}m recebido muita aten{\c{c}}{\~{a}}o pela sua alta taxa de acur{\'{a}}cia preditiva, no entanto n{\~{a}}o {\'{e}} f{\'{a}}cil compreender como elas alcan{\c{c}}am as suas decis{\~{o}}es. Neste artigo um conjunto de dados de cr{\'{e}}dito {\'{e}} analisado usando a t{\'{e}}cnica de extra{\c{c}}{\~{a}}o de regras NeuroRule e o software WEKA para a extra{\c{c}}{\~{a}}o de regras a partir de uma Rede Neural treinada. Os resultados foram considerados bastante satisfat{\'{o}}rios alcan{\c{c}}ando mais de 80{\%} de acur{\'{a}}cia quanto {\`{a}} concess{\~{a}}o (ou n{\~{a}}o) de cr{\'{e}}dito banc{\'{a}}rio em todas as simula{\c{c}}{\~{o}}es.},
author = {Steiner, Maria Teresinha Arns and Nievola, J{\'{u}}lio Cesar and Soma, Nei Yoshihiro and Shimizu, Tamio and {Steiner Neto}, Pedro Jos{\'{e}}},
doi = {10.1590/S0101-74382007000300002},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/v27n3a02{\_}regras{\_}rNA{\_}tomada{\_}decisao{\_}creditoBancario.pdf:pdf},
issn = {0101-7438},
journal = {Pesquisa Operacional},
pages = {407--426},
title = {{Extra{\c{c}}{\~{a}}o de regras de classifica{\c{c}}{\~{a}}o a partir de redes neurais para aux{\'{i}}lio {\`{a}} tomada de decis{\~{a}}o na concess{\~{a}}o de cr{\'{e}}dito banc{\'{a}}rio}},
url = {http://www.scielo.br/scielo.php?pid=S0101-74382007000300002{\&}script=sci{\_}arttext{\&}tlng=pt},
volume = {27},
year = {2007}
}
@article{Monteiro2013,
author = {Monteiro, Felipe},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/c4.5vsMLP-2013 CLAGTEE{\_}submission{\_}141.pdf:pdf},
number = {October},
title = {{aplicados a Avalia{\c{c}}{\~{a}}o da Seguran{\c{c}}a Din{\^{a}}mica e}},
year = {2013}
}
@article{Lopes,
author = {Lopes, Lucas A and Machado, Vinicius P and Rabelo, Ricardo De A L},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/artigo{\_}lopes.pdf:pdf},
title = {{Automatic Labeling of Groupings through Supervised Machine Learning}}
}
@article{Prati2006,
abstract = {Machine learning algorithms are often the most appropriate algorithms for a great variety of data mining applications. However, most machine lear- ning research to date has mainly dealt with the well-circumscribed problem of finding a model (generally a classifier) given a single, small and relatively clean dataset in the attribute-value form, where the attributes have previ- ously been chosen to facilitate learning. Furthermore, the end-goal is simple and well-defined, such as accurate classifiers in the classification problem. Data mining opens up new directions for machine learning research, and lends new urgency to others. With data mining, machine learning is now re- moving each one of these constraints. Therefore, machine learning's many valuable contributions to data mining are reciprocated by the latter's invi- gorating effect on it. In this thesis, we explore this interaction by proposing new solutions to some problems due to the application of machine learning algorithms to data mining applications. More specifically, we contribute to the following problems. New approaches to rule learning. In this category, we propose two new methods for rule learning. In the first one, we propose a new method for finding exceptions to general rules. The second one is a rule selection algorithm based on the ROC graph. Rules come from an external larger set of rules and the algorithm performs a selection step based on the current convex hull in the ROC graph. Proportion of examples among classes. We investigated several as- pects related to this issue. Firstly, we carried out a series of experiments on artificial data sets in order to verify our hypothesis that overlapping among classes is a complicating factor in highly skewed data sets. We also carried out a broadly experimental analysis with several methods (some of them proposed by us) that artificially balance skewed datasets. Our experiments show that, in general, over-sampling methods perform better than under- sampling methods. Finally, we investigated the relationship between class imbalance and small disjuncts, as well as the influence of the proportion of examples among classes in the process of labelling unlabelled cases in the semi-supervised learning algorithm Co-training. New method for combining rankings. We propose a new method cal- led BORDARANK to construct ensembles of rankings based on borda count voting, which could be applied whenever only the rankings are available. Results show an improvement upon the base-rankings constructed by ta- king into account the ordering given by classifiers which output continuous- valued scores, as well as a comparable performance with the fusion of such scores.},
author = {Prati, Ronaldo Cristiano},
doi = {10.11606/T.55.2006.tde-01092006-155445},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/RonaldoPratiTese.pdf:pdf},
pages = {191},
title = {{Novas abordagens em aprendizado de m{\'{a}}quina para a gera{\c{c}}{\~{a}}o de regras, classes desbalanceadas e ordena{\c{c}}{\~{a}}o de casos - Tese de Doutorado}},
year = {2006}
}
@article{Catlett2006a,
author = {Catlett, J},
file = {:home/tarcisio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Catlett - 2006 - Into Ordered Discrete Attributes(2).pdf:pdf;:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/carlet1991.pdf:pdf},
keywords = {discretisation,empirical concept learning,induction of decision trees},
number = {1989},
pages = {2006},
title = {{Into Ordered Discrete Attributes}},
volume = {3},
year = {2006}
}
@book{Mitchell1997,
abstract = {Um texto introdut{\'{o}}rio sobre abordagens prim{\'{a}}rias para a aprendizagem de m{\'{a}}quinas e o estudo de algoritmos de computador que melhoram automaticamente atrav{\'{e}}s da experi{\^{e}}ncia. Introduza conceitos b{\'{a}}sicos de estat{\'{i}}stica, intelig{\^{e}}ncia artificial, teoria da informa{\c{c}}{\~{a}}o e outras disciplinas, conforme necess{\'{a}}rio, com cobertura equilibrada de teoria e pr{\'{a}}tica e apresenta algoritmos importantes com ilustra{\c{c}}{\~{o}}es de seu uso. Inclui exerc{\'{i}}cios de cap{\'{i}}tulo. Conjuntos de dados on-line e implementa{\c{c}}{\~{o}}es de v{\'{a}}rios algoritmos est{\~{a}}o dispon{\'{i}}veis em um site. Nenhum conhecimento pr{\'{e}}vio em intelig{\^{e}}ncia artificial ou estat{\'{i}}stica {\'{e}} assumido. Para graduados avan{\c{c}}ados e estudantes de p{\'{o}}s-gradua{\c{c}}{\~{a}}o em inform{\'{a}}tica, engenharia, estat{\'{i}}stica e ci{\^{e}}ncias sociais, bem como profissionais de software.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Mitchell, Tom M.},
booktitle = {McGraw-Hill Science/Engineering/Math},
doi = {10.1007/978-3-540-75488-6_2},
eprint = {0-387-31073-8},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/McGrawHill{\_}-{\_}Machine{\_}Learning{\_}-Tom{\_}Mitchell.pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
pages = {432},
pmid = {18292226},
publisher = {McGraw-Hill Science/Engineering/Math},
title = {{Machine learning}},
year = {1997}
}
@book{Bishop2013,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2013}
}
@article{Morasca2002,
abstract = {Classification trees have been successfully used in several application fields. However, continuous attributes cannot be used directly when building classification trees, but they must be first discretized with clustering techniques, which require some degree of subjectivity. We propose an approach to build classification trees that does not require the discretization of the continuous attributes. The approach is an extension of existing methods for building classification trees and is based on the information gain yielded by discrete and continuous attributes. Data from a software development case study are analyzed with both the proposed approach and C4.5 to show the approach's applicability and benefits over C4.5. Copyright 2002 ACM.},
author = {Morasca, Sandro and Chimiche, Scienze and Matematiche, Fisiche},
doi = {10.1145/568760.568832},
file = {:mnt/home{\_}zenwalk/tarcisio/Documentos/mestrado{\_}UFPI/dissertacao/build{\_}Dissertacao/Textos/artigos/p417-morasca.pdf:pdf},
isbn = {1581135564},
keywords = {5,c4,continuous attributes,decision trees,id3},
pages = {0--7},
title = {{A Proposal for Using Continuous Attributes in Classification Trees}},
year = {2002}
}
@book{runkler2012,
title={Models and Algorithms for Intelligent Data Analysis},
author={Thomas A. Runkler},
isbn={978-3-658-14075-5},
series={Data Analytics},
publisher={Springer Vieweg},
year={2016},
edition={2},
copyright={Springer Fachmedien Wiesbaden}
}
@book{breiman1984,
  title={Classification and Regression Trees},
  author={Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn={9780412048418},
  lccn={83019708},
  series={The Wadsworth and Brooks-Cole statistics-probability series},
  url={https://books.google.com.br/books?id=JwQx-WOmSyQC},
  year={1984},
  publisher={Taylor \& Francis}
}
@book{yohannes1999classification,
  title={Classification and Regression Trees, CART: A User Manual for Identifying Indicators of Vulnerability to Famine and Chronic Food Insecurity},
  author={Yohannes, Y. and Webb, P.},
  isbn={9780896293373},
  lccn={99022708},
  series={Microcomputers in policy research},
  url={https://books.google.com.br/books?id=7iuq4ikyNdoC},
  year={1999},
  publisher={International Food Policy Research Institute}
}

@incollection{Evett:1989,
 author = {Evett, I. W. and Spiehler, E. J.},
 chapter = {Rule Induction in Forensic Science},
 title = {Knowledge Based Systems},
 editor = {Duffin, P. H.},
 year = {1988},
 isbn = {0-470-21260-8},
 pages = {152--160},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=67040.67055},
 acmid = {67055},
 publisher = {Halsted Press},
 address = {New York, NY, USA},
} 
